{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.26.0 in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (4.26.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (from transformers==4.26.0) (3.12.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (from transformers==4.26.0) (4.65.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (from transformers==4.26.0) (23.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (from transformers==4.26.0) (0.13.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (from transformers==4.26.0) (1.24.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (from transformers==4.26.0) (0.14.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (from transformers==4.26.0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (from transformers==4.26.0) (2023.3.23)\n",
      "Requirement already satisfied: requests in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (from transformers==4.26.0) (2.28.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.0) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.0) (4.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (from tqdm>=4.27->transformers==4.26.0) (0.4.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (from requests->transformers==4.26.0) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (from requests->transformers==4.26.0) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (from requests->transformers==4.26.0) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (from requests->transformers==4.26.0) (3.4)\n",
      "Requirement already satisfied: accelerate==0.16.0 in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (0.16.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (from accelerate==0.16.0) (23.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (from accelerate==0.16.0) (2.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (from accelerate==0.16.0) (1.24.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (from accelerate==0.16.0) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (from accelerate==0.16.0) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (from torch>=1.4.0->accelerate==0.16.0) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (from torch>=1.4.0->accelerate==0.16.0) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (from torch>=1.4.0->accelerate==0.16.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (from torch>=1.4.0->accelerate==0.16.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (from torch>=1.4.0->accelerate==0.16.0) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (from jinja2->torch>=1.4.0->accelerate==0.16.0) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (from sympy->torch>=1.4.0->accelerate==0.16.0) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.26.0\n",
    "!pip install accelerate==0.16.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\william_H\\anaconda3\\envs\\hw7\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "from transformers import AdamW, BertForQuestionAnswering, BertTokenizerFast\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\", 1) if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "def same_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "same_seeds(7414)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate==0.2.0\n",
      "  Using cached accelerate-0.2.0-py3-none-any.whl (47 kB)\n",
      "Requirement already satisfied: pyaml>=20.4.0 in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (from accelerate==0.2.0) (21.10.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (from accelerate==0.2.0) (2.0.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (from pyaml>=20.4.0->accelerate==0.2.0) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (from torch>=1.4.0->accelerate==0.2.0) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (from torch>=1.4.0->accelerate==0.2.0) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (from torch>=1.4.0->accelerate==0.2.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (from torch>=1.4.0->accelerate==0.2.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (from torch>=1.4.0->accelerate==0.2.0) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (from jinja2->torch>=1.4.0->accelerate==0.2.0) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\william_h\\anaconda3\\envs\\hw7\\lib\\site-packages (from sympy->torch>=1.4.0->accelerate==0.2.0) (1.3.0)\n",
      "Installing collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.16.0\n",
      "    Uninstalling accelerate-0.16.0:\n",
      "      Successfully uninstalled accelerate-0.16.0\n",
      "Successfully installed accelerate-0.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Change \"fp16_training\" to True to support automatic mixed precision training (fp16)\n",
    "fp16_training = True\n",
    "fp16 = True\n",
    "if fp16_training:\n",
    "    %pip install accelerate==0.2.0\n",
    "    from accelerate import Accelerator\n",
    "    accelerator = Accelerator(fp16)\n",
    "    device = accelerator.device\n",
    "\n",
    "# Documentation for the toolkit:  https://huggingface.co/docs/accelerate/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(\"luhua/chinese_pretrain_mrc_macbert_large\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"luhua/chinese_pretrain_mrc_macbert_large\")\n",
    "\n",
    "# You can safely ignore the warning message (it pops up because new prediction heads for QA are initialized randomly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file):\n",
    "    with open(file, 'r', encoding=\"utf-8\") as reader:\n",
    "        data = json.load(reader)\n",
    "    return data[\"questions\"], data[\"paragraphs\"]\n",
    "\n",
    "train_questions, train_paragraphs = read_data(\"./hw7_train.json\")\n",
    "dev_questions, dev_paragraphs = read_data(\"./hw7_dev.json\")\n",
    "test_questions, test_paragraphs = read_data(\"./hw7_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize questions and paragraphs separately\n",
    "# 「add_special_tokens」 is set to False since special tokens will be added when tokenized questions and paragraphs are combined in datset __getitem__ \n",
    "\n",
    "train_questions_tokenized = tokenizer([train_question[\"question_text\"] for train_question in train_questions], add_special_tokens=False)\n",
    "dev_questions_tokenized = tokenizer([dev_question[\"question_text\"] for dev_question in dev_questions], add_special_tokens=False)\n",
    "test_questions_tokenized = tokenizer([test_question[\"question_text\"] for test_question in test_questions], add_special_tokens=False) \n",
    "\n",
    "train_paragraphs_tokenized = tokenizer(train_paragraphs, add_special_tokens=False)\n",
    "dev_paragraphs_tokenized = tokenizer(dev_paragraphs, add_special_tokens=False)\n",
    "test_paragraphs_tokenized = tokenizer(test_paragraphs, add_special_tokens=False)\n",
    "\n",
    "# You can safely ignore the warning message as tokenized sequences will be futher processed in datset __getitem__ before passing to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_STRIDE = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QA_Dataset(Dataset):\n",
    "    def __init__(self, split, questions, tokenized_questions, tokenized_paragraphs):\n",
    "        self.split = split\n",
    "        self.questions = questions\n",
    "        self.tokenized_questions = tokenized_questions\n",
    "        self.tokenized_paragraphs = tokenized_paragraphs\n",
    "        self.max_question_len = 50\n",
    "        self.max_paragraph_len = 400\n",
    "        \n",
    "        ##### TODO: Change value of doc_stride #####\n",
    "        self.doc_stride = int(0.2 * self.max_paragraph_len)\n",
    "        ############################################\n",
    "        global DOC_STRIDE\n",
    "        DOC_STRIDE = self.doc_stride\n",
    "        ############################################\n",
    "        # Input sequence length = [CLS] + question + [SEP] + paragraph + [SEP]\n",
    "        self.max_seq_len = 1 + self.max_question_len + 1 + self.max_paragraph_len + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question = self.questions[idx]\n",
    "        tokenized_question = self.tokenized_questions[idx]\n",
    "        tokenized_paragraph = self.tokenized_paragraphs[question[\"paragraph_id\"]]\n",
    "\n",
    "        ##### TODO: Preprocessing #####\n",
    "        # Hint: How to prevent model from learning something it should not learn\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            # Convert answer's start/end positions in paragraph_text to start/end positions in tokenized_paragraph  \n",
    "            answer_start_token = tokenized_paragraph.char_to_token(question[\"answer_start\"])\n",
    "            answer_end_token = tokenized_paragraph.char_to_token(question[\"answer_end\"])\n",
    "\n",
    "#             # A single window is obtained by slicing the portion of paragraph containing the answer\n",
    "#             mid = (answer_start_token + answer_end_token) // 2\n",
    "#             paragraph_start = max(0, min(mid - self.max_paragraph_len // 2, len(tokenized_paragraph) - self.max_paragraph_len))\n",
    "#             paragraph_end = paragraph_start + self.max_paragraph_len\n",
    "            \n",
    "            # A single window is obtained by slicing the portion of paragraph containing the answer\n",
    "            mid = (answer_start_token + answer_end_token) // 2\n",
    "            prefix_len = int(random.random() * self.max_paragraph_len)\n",
    "            postfix_len = self.max_paragraph_len - prefix_len\n",
    "            paragraph_start, paragraph_end = mid - prefix_len, mid + postfix_len\n",
    "            if paragraph_start < 0:\n",
    "                paragraph_end -= paragraph_start\n",
    "                paragraph_start = 0\n",
    "            if paragraph_end >= len(tokenized_paragraph):\n",
    "                paragraph_end = len(tokenized_paragraph) - 1\n",
    "            \n",
    "            # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n",
    "            input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102] \n",
    "            input_ids_paragraph = tokenized_paragraph.ids[paragraph_start : paragraph_end] + [102]\n",
    "            \n",
    "            # Convert answer's start/end positions in tokenized_paragraph to start/end positions in the window  \n",
    "            answer_start_token += len(input_ids_question) - paragraph_start\n",
    "            answer_end_token += len(input_ids_question) - paragraph_start\n",
    "            \n",
    "            # Pad sequence and obtain inputs to model \n",
    "            input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n",
    "            return torch.tensor(input_ids), torch.tensor(token_type_ids), torch.tensor(attention_mask), answer_start_token, answer_end_token\n",
    "\n",
    "        # Validation/Testing\n",
    "        else:\n",
    "            input_ids_list, token_type_ids_list, attention_mask_list = [], [], []\n",
    "            \n",
    "            # Paragraph is split into several windows, each with start positions separated by step \"doc_stride\"\n",
    "            for i in range(0, len(tokenized_paragraph), self.doc_stride):\n",
    "                \n",
    "                # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n",
    "                input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102]\n",
    "                input_ids_paragraph = tokenized_paragraph.ids[i : i + self.max_paragraph_len] + [102]\n",
    "                \n",
    "                # Pad sequence and obtain inputs to model\n",
    "                input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n",
    "                \n",
    "                input_ids_list.append(input_ids)\n",
    "                token_type_ids_list.append(token_type_ids)\n",
    "                attention_mask_list.append(attention_mask)\n",
    "            \n",
    "            return torch.tensor(input_ids_list), torch.tensor(token_type_ids_list), torch.tensor(attention_mask_list)\n",
    "\n",
    "    def padding(self, input_ids_question, input_ids_paragraph):\n",
    "        # Pad zeros if sequence length is shorter than max_seq_len\n",
    "        padding_len = self.max_seq_len - len(input_ids_question) - len(input_ids_paragraph)\n",
    "        # Indices of input sequence tokens in the vocabulary\n",
    "        input_ids = input_ids_question + input_ids_paragraph + [0] * padding_len\n",
    "        # Segment token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]\n",
    "        token_type_ids = [0] * len(input_ids_question) + [1] * len(input_ids_paragraph) + [0] * padding_len\n",
    "        # Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]\n",
    "        attention_mask = [1] * (len(input_ids_question) + len(input_ids_paragraph)) + [0] * padding_len\n",
    "        \n",
    "        return input_ids, token_type_ids, attention_mask\n",
    "\n",
    "train_set = QA_Dataset(\"train\", train_questions, train_questions_tokenized, train_paragraphs_tokenized)\n",
    "dev_set = QA_Dataset(\"dev\", dev_questions, dev_questions_tokenized, dev_paragraphs_tokenized)\n",
    "test_set = QA_Dataset(\"test\", test_questions, test_questions_tokenized, test_paragraphs_tokenized)\n",
    "\n",
    "train_batch_size = 4\n",
    "\n",
    "# Note: Do NOT change batch size of dev_loader / test_loader !\n",
    "# Although batch size=1, it is actually a batch consisting of several windows from the same QA pair\n",
    "train_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True, pin_memory=True)\n",
    "dev_loader = DataLoader(dev_set, batch_size=1, shuffle=False, pin_memory=True)\n",
    "test_loader = DataLoader(test_set, batch_size=1, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data, output, paragraph, paragraph_tokenized):\n",
    "    ##### TODO: Postprocessing #####\n",
    "    # There is a bug and room for improvement in postprocessing \n",
    "    # Hint: Open your prediction file to see what is wrong \n",
    "    \n",
    "    answer = ''\n",
    "    max_prob = float('-inf')\n",
    "    num_of_windows = data[0].shape[1]\n",
    "    \n",
    "    paragraph_start_index = 0\n",
    "    paragraph_end_index = 0\n",
    "    \n",
    "    for k in range(num_of_windows):\n",
    "        # Obtain answer by choosing the most probable start position / end position\n",
    "#         start_prob, start_index = torch.max(output.start_logits[k], dim=0)\n",
    "#         end_prob, end_index = torch.max(output.end_logits[k], dim=0)\n",
    "        \n",
    "#         # Probability of answer is calculated as sum of start_prob and end_prob\n",
    "#         prob = start_prob + end_prob\n",
    "\n",
    "        mask = (data[1][0][k].bool() & data[2][0][k].bool()).to(device)\n",
    "    \n",
    "        masked_output_start = torch.masked_select(output.start_logits[k], mask)\n",
    "        masked_output_start = masked_output_start[:-1]\n",
    "        \n",
    "        start_prob, start_index = torch.max(masked_output_start, dim=0)\n",
    "        \n",
    "        masked_output_end = torch.masked_select(output.end_logits[k], mask)\n",
    "        masked_output_end = masked_output_end[start_index: -1]\n",
    "        \n",
    "        end_prob, end_index = torch.max(masked_output_end, dim=0)\n",
    "        \n",
    "        end_index += start_index\n",
    "        \n",
    "        # Probability of answer is calculated as sum of start_prob and end_prob\n",
    "        prob = start_prob + end_prob\n",
    "        masked_data = torch.masked_select(data[0][0][k].to(device), mask)[:-1]\n",
    "        \n",
    "        # Replace answer if calculated probability is larger than previous windows\n",
    "        if (prob > max_prob) and (start_index <= end_index <= (start_index + 50)):\n",
    "            max_prob = prob\n",
    "            paragraph_start_index = start_index.item() + (DOC_STRIDE * k)\n",
    "            paragraph_end_index = end_index.item() + (DOC_STRIDE * k)\n",
    "            # Convert tokens to chars (e.g. [1920, 7032] --> \"大 金\")\n",
    "#             answer = tokenizer.decode(data[0][0][k][start_index : end_index + 1])\n",
    "            answer = tokenizer.decode(masked_data[start_index : end_index + 1])\n",
    "            \n",
    "#     # 轉換 [UNK]        \n",
    "#     if \"[UNK]\" in answer:\n",
    "#         # 原始答案 #\n",
    "#         print(f\"原始答案: {answer}\")\n",
    "#         ##########\n",
    "#         char_count = 0\n",
    "#         start_flag = False\n",
    "\n",
    "#         for i, token in enumerate(paragraph_tokenized):\n",
    "#             if token in ('[UNK]', '[CLS]', '[SEP]'):\n",
    "#                 if i == paragraph_start_index:\n",
    "#                     new_start = char_count\n",
    "#                 if i == paragraph_end_index:\n",
    "#                     new_end = char_count\n",
    "#                 char_count += 1\n",
    "#             else:\n",
    "#                 for char in token:\n",
    "#                     if i == paragraph_start_index and not start_flag:\n",
    "#                         new_start = char_count\n",
    "#                         start_flag = True\n",
    "#                     if i == paragraph_end_index:\n",
    "#                         new_end = char_count\n",
    "#                     if char == \"#\":\n",
    "#                         continue\n",
    "#                     else:\n",
    "#                         while char_count < len(paragraph) and char != paragraph[char_count]:\n",
    "#                             char_count += 1\n",
    "#                         char_count += 1\n",
    "            \n",
    "#         answer = paragraph[new_start: new_end+1]\n",
    "#         print(f\"修正後答案: {answer}\")\n",
    "#         print(paragraph_start_index, paragraph_end_index)\n",
    "#         print(new_start, new_end)\n",
    "#         print(\"-\"*50)\n",
    "\n",
    "    \n",
    "    ##########\n",
    "    char_count = 0\n",
    "    start_flag = False\n",
    "\n",
    "    for i, token in enumerate(paragraph_tokenized):\n",
    "        if token in ('[UNK]', '[CLS]', '[SEP]'):\n",
    "            if i == paragraph_start_index:\n",
    "                new_start = char_count\n",
    "            if i == paragraph_end_index:\n",
    "                new_end = char_count\n",
    "            char_count += 1\n",
    "        else:\n",
    "            for char in token:\n",
    "                if i == paragraph_start_index and not start_flag:\n",
    "                    new_start = char_count\n",
    "                    start_flag = True\n",
    "                if i == paragraph_end_index:\n",
    "                    new_end = char_count\n",
    "                if char == \"#\":\n",
    "                    continue\n",
    "                else:\n",
    "                    while char_count < len(paragraph) and char != paragraph[char_count]:\n",
    "                        char_count += 1\n",
    "                    char_count += 1\n",
    "    # 後處理\n",
    "    # 1.轉換 [UNK]        \n",
    "    if \"[UNK]\" in answer:\n",
    "        # 原始答案 #\n",
    "        print(f\"原始答案: {answer}\")\n",
    "        # 修正後答案 #\n",
    "        answer = paragraph[new_start: new_end+1]\n",
    "        print(f\"修正後答案: {answer}\")\n",
    "        print(\"-\"*50)\n",
    "\n",
    "###########################################################\n",
    "    \n",
    "#     # 2.把沒抓到的引號補回來\n",
    "#     # 尾巴引號漏掉\n",
    "#     if (new_end+1) < len(paragraph):\n",
    "#         if paragraph[new_start] == \"「\" and paragraph[new_end+1] == \"」\":\n",
    "#             answer = paragraph[new_start: new_end+2]\n",
    "#     # 頭尾引號都漏掉\n",
    "#     if new_start > 0 and (new_end+1) < len(paragraph):\n",
    "#         if paragraph[new_start-1] == \"「\" and paragraph[new_end+1] == \"」\":\n",
    "#             answer = paragraph[new_start-1: new_end+2]\n",
    "#     # 尾巴隔一個標點符號才接引號\n",
    "#     if (new_end+2) < len(paragraph):\n",
    "#         if paragraph[new_start] == \"「\" and paragraph[new_end+2] == \"」\":\n",
    "#             answer = paragraph[new_start: new_end+3]\n",
    "#     if new_start > 0 and (new_end+2) < len(paragraph):\n",
    "#         if paragraph[new_start-1] == \"「\" and paragraph[new_end+2] == \"」\":\n",
    "#             answer = paragraph[new_start-1: new_end+3]\n",
    "    \n",
    "#     # 3.把「為了」與「因為」抓回來\n",
    "#     if new_start > 1:\n",
    "#         if paragraph[new_start-2: new_start] == \"為了\":\n",
    "#             answer = paragraph[new_start-2: new_end+1]\n",
    "#         if paragraph[new_start-2: new_start] == \"因為\":\n",
    "#             answer = paragraph[new_start-2: new_end+1]\n",
    "\n",
    "    \n",
    "#     if new_start > 0:\n",
    "#         if paragraph[new_start-1] == \"因\":\n",
    "#             answer = paragraph[new_start-1: new_end+1]\n",
    "\n",
    "###########################################################\n",
    "        \n",
    "    # 最後移除空格 (e.g. \"大 金\" --> \"大金\")\n",
    "    answer = answer.replace(' ', '')\n",
    "    \n",
    "###########################################################\n",
    "\n",
    "#     # 把「為了」與「因為」拿掉\n",
    "#     if len(answer) > 2:\n",
    "#         if answer[:2] == \"為了\" or answer[:2] == \"因為\":\n",
    "#             answer = answer[2:]\n",
    "    \n",
    "#     # 把「」拿掉\n",
    "#     if len(answer) > 2:\n",
    "#         if answer[0] == \"「\" and answer[-1] == \"」\":\n",
    "#             answer = answer[1:-1]\n",
    "    \n",
    "###########################################################\n",
    "\n",
    "    if len(answer) > 1:\n",
    "        if \"「\" not in answer and answer[-1] == \"」\":\n",
    "            answer = answer[:-1]\n",
    "#     if 3 > len(answer) > 1:\n",
    "#         if answer[0] == \"第\":\n",
    "#             i = 1\n",
    "#             while i < len(answer) and answer[i] in list(\"123456789一二三四五六七八九\"):\n",
    "#                 i += 1\n",
    "#             answer = answer[1:i]\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\william_H\\anaconda3\\envs\\hw7\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/6730 [00:02<5:30:47,  2.95s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 12.00 GiB total capacity; 10.71 GiB already allocated; 0 bytes free; 11.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 33\u001b[0m\n\u001b[0;32m     29\u001b[0m data \u001b[39m=\u001b[39m [i\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m data]\n\u001b[0;32m     31\u001b[0m \u001b[39m# Model inputs: input_ids, token_type_ids, attention_mask, start_positions, end_positions (Note: only \"input_ids\" is mandatory)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39m# Model outputs: start_logits, end_logits, loss (return when start_positions/end_positions are provided)  \u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m output \u001b[39m=\u001b[39m model(input_ids\u001b[39m=\u001b[39;49mdata[\u001b[39m0\u001b[39;49m], token_type_ids\u001b[39m=\u001b[39;49mdata[\u001b[39m1\u001b[39;49m], attention_mask\u001b[39m=\u001b[39;49mdata[\u001b[39m2\u001b[39;49m], start_positions\u001b[39m=\u001b[39;49mdata[\u001b[39m3\u001b[39;49m], end_positions\u001b[39m=\u001b[39;49mdata[\u001b[39m4\u001b[39;49m])\n\u001b[0;32m     35\u001b[0m \u001b[39m# Choose the most probable start position / end position\u001b[39;00m\n\u001b[0;32m     36\u001b[0m start_index \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(output\u001b[39m.\u001b[39mstart_logits, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\william_H\\anaconda3\\envs\\hw7\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\william_H\\anaconda3\\envs\\hw7\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1851\u001b[0m, in \u001b[0;36mBertForQuestionAnswering.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1839\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1840\u001b[0m \u001b[39mstart_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1841\u001b[0m \u001b[39m    Labels for position (index) of the start of the labelled span for computing the token classification loss.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1847\u001b[0m \u001b[39m    are not taken into account for computing the loss.\u001b[39;00m\n\u001b[0;32m   1848\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1849\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1851\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[0;32m   1852\u001b[0m     input_ids,\n\u001b[0;32m   1853\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1854\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1855\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1856\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1857\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1858\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1859\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1860\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1861\u001b[0m )\n\u001b[0;32m   1863\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1865\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqa_outputs(sequence_output)\n",
      "File \u001b[1;32mc:\\Users\\william_H\\anaconda3\\envs\\hw7\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\william_H\\anaconda3\\envs\\hw7\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1019\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1010\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1012\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m   1013\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1014\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1017\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1018\u001b[0m )\n\u001b[1;32m-> 1019\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m   1020\u001b[0m     embedding_output,\n\u001b[0;32m   1021\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m   1022\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1023\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1024\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   1025\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1026\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1027\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1028\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1029\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1030\u001b[0m )\n\u001b[0;32m   1031\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1032\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\william_H\\anaconda3\\envs\\hw7\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\william_H\\anaconda3\\envs\\hw7\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:609\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    600\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    601\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    602\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    606\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    607\u001b[0m     )\n\u001b[0;32m    608\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 609\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    610\u001b[0m         hidden_states,\n\u001b[0;32m    611\u001b[0m         attention_mask,\n\u001b[0;32m    612\u001b[0m         layer_head_mask,\n\u001b[0;32m    613\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    614\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    615\u001b[0m         past_key_value,\n\u001b[0;32m    616\u001b[0m         output_attentions,\n\u001b[0;32m    617\u001b[0m     )\n\u001b[0;32m    619\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    620\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\william_H\\anaconda3\\envs\\hw7\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\william_H\\anaconda3\\envs\\hw7\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:495\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    484\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    485\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    492\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m    493\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    494\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 495\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    496\u001b[0m         hidden_states,\n\u001b[0;32m    497\u001b[0m         attention_mask,\n\u001b[0;32m    498\u001b[0m         head_mask,\n\u001b[0;32m    499\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    500\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    501\u001b[0m     )\n\u001b[0;32m    502\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    504\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\william_H\\anaconda3\\envs\\hw7\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\william_H\\anaconda3\\envs\\hw7\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:425\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    416\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    417\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    423\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    424\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 425\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[0;32m    426\u001b[0m         hidden_states,\n\u001b[0;32m    427\u001b[0m         attention_mask,\n\u001b[0;32m    428\u001b[0m         head_mask,\n\u001b[0;32m    429\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    430\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    431\u001b[0m         past_key_value,\n\u001b[0;32m    432\u001b[0m         output_attentions,\n\u001b[0;32m    433\u001b[0m     )\n\u001b[0;32m    434\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[0;32m    435\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\william_H\\anaconda3\\envs\\hw7\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\william_H\\anaconda3\\envs\\hw7\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:323\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    320\u001b[0m     past_key_value \u001b[39m=\u001b[39m (key_layer, value_layer)\n\u001b[0;32m    322\u001b[0m \u001b[39m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[1;32m--> 323\u001b[0m attention_scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(query_layer, key_layer\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m))\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrelative_key\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrelative_key_query\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    326\u001b[0m     query_length, key_length \u001b[39m=\u001b[39m query_layer\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m], key_layer\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 12.00 GiB total capacity; 10.71 GiB already allocated; 0 bytes free; 11.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "num_epoch = 3  # 3\n",
    "validation = True  # True\n",
    "logging_step = 500\n",
    "learning_rate = 5e-6\n",
    "# accum_iter = 8\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "##### TODO: Apply linear learning rate decay #####\n",
    "total_steps = len(train_loader) * num_epoch\n",
    "# warmup_steps = int(0.05 * total_steps)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=total_steps)\n",
    "##################################################\n",
    "\n",
    "if fp16_training:\n",
    "    model, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader) \n",
    "\n",
    "model.train()\n",
    "\n",
    "print(\"Start Training ...\")\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    step = 1\n",
    "    train_loss = train_acc = 0\n",
    "    \n",
    "    for batch_idx, data in enumerate(tqdm(train_loader)):\n",
    "        # Load all data into GPU\n",
    "        data = [i.to(device) for i in data]\n",
    "        \n",
    "        # Model inputs: input_ids, token_type_ids, attention_mask, start_positions, end_positions (Note: only \"input_ids\" is mandatory)\n",
    "        # Model outputs: start_logits, end_logits, loss (return when start_positions/end_positions are provided)  \n",
    "        output = model(input_ids=data[0], token_type_ids=data[1], attention_mask=data[2], start_positions=data[3], end_positions=data[4])\n",
    "\n",
    "        # Choose the most probable start position / end position\n",
    "        start_index = torch.argmax(output.start_logits, dim=1)\n",
    "        end_index = torch.argmax(output.end_logits, dim=1)\n",
    "\n",
    "        # Prediction is correct only if both start_index and end_index are correct\n",
    "        train_acc += ((start_index == data[3]) & (end_index == data[4])).float().mean()\n",
    "        train_loss += output.loss\n",
    "\n",
    "        if fp16_training:\n",
    "            accelerator.backward(output.loss)\n",
    "        else:\n",
    "            output.loss.backward()\n",
    "\n",
    "        ##### TODO: Apply linear learning rate decay #####\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        step += 1\n",
    "        ##################################################\n",
    "        \n",
    "        # Print training loss and accuracy over past logging step\n",
    "        if step % logging_step == 0:\n",
    "            print(f\"Epoch {epoch + 1} | Step {step} | loss = {train_loss.item() / logging_step:.3f}, acc = {train_acc / logging_step:.3f}\")\n",
    "            train_loss = train_acc = 0\n",
    "            \n",
    "    if validation:\n",
    "        print(\"Evaluating Dev Set ...\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            dev_acc = 0\n",
    "            for i, data in enumerate(tqdm(dev_loader)):\n",
    "                output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n",
    "                       attention_mask=data[2].squeeze(dim=0).to(device))\n",
    "                # prediction is correct only if answer text exactly matches\n",
    "                dev_acc += evaluate(data, output, dev_paragraphs[dev_questions[i]['paragraph_id']], dev_paragraphs_tokenized[dev_questions[i]['paragraph_id']].tokens) == dev_questions[i][\"answer_text\"]\n",
    "            print(f\"Validation | Epoch {epoch + 1} | acc = {dev_acc / len(dev_loader):.3f}\")\n",
    "        model.train()\n",
    "\n",
    "# Save a model and its configuration file to the directory 「saved_model」 \n",
    "# i.e. there are two files under the direcory 「saved_model」: 「pytorch_model.bin」 and 「config.json」\n",
    "# Saved model can be re-loaded using 「model = BertForQuestionAnswering.from_pretrained(\"saved_model\")」\n",
    "print(\"Saving Model ...\")\n",
    "model_save_dir = \"./testmodel8\" \n",
    "model.save_pretrained(model_save_dir)\n",
    "# part1 - 加速 (有幫助加速) - simple submit\n",
    "    # 991/991 [05:52<00:00, 3.45it/s]\n",
    "    # to\n",
    "    # 991/991 [03:00<00:00, 6.08it/s]\n",
    "\n",
    "    # Epoch 1 | Step 900 | loss = 0.586, acc = 0.748\n",
    "    # Validation | Epoch 1 | acc = 0.509\n",
    "    # to\n",
    "    # Epoch 1 | Step 900 | loss = 0.576, acc = 0.759\n",
    "    # Validation | Epoch 1 | acc = 0.538\n",
    "\n",
    "# part2 - LR Schedule (訓練結果有變好，驗證沒有)\n",
    "    # Validation | Epoch 1 | acc = 0.539\n",
    "    \n",
    "# part3 - doc_stride to 0.5 (變好很多)\n",
    "    # Validation | Epoch 1 | acc = 0.667\n",
    "\n",
    "# part4 - random split training answer window (變好很多)\n",
    "    # Validation | Epoch 1 | acc = 0.733\n",
    "    \n",
    "# part5 - Doc Length from 150 to 300 (好一點點)\n",
    "    # Validation | Epoch 1 | acc = 0.727\n",
    "    \n",
    "# part6 - 換模型\n",
    "    # Validation | Epoch 1 | acc = 0.750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating Test Set ...\")\n",
    "\n",
    "result = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(tqdm(test_loader)):\n",
    "        output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n",
    "                       attention_mask=data[2].squeeze(dim=0).to(device))\n",
    "        result.append(evaluate(data, output, test_paragraphs[test_questions[i]['paragraph_id']], test_paragraphs_tokenized[test_questions[i]['paragraph_id']].tokens))\n",
    "\n",
    "result_file = \"./testresult8.csv\"\n",
    "with open(result_file, 'w') as f:\n",
    "    f.write(\"ID,Answer\\n\")\n",
    "    for i, test_question in enumerate(test_questions):\n",
    "        # Replace commas in answers with empty strings (since csv is separated by comma)\n",
    "        # Answers in kaggle are processed in the same way\n",
    "        f.write(f\"{test_question['id']},{result[i].replace(',','')}\\n\")\n",
    "\n",
    "print(f\"Completed! Result is in {result_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
