{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFEKWoh3p1Mv"
      },
      "source": [
        "# Homework Description\n",
        "- English to Chinese (Traditional) Translation\n",
        "  - Input: an English sentence         (e.g.\t\ttom is a student .)\n",
        "  - Output: the Chinese translation  (e.g. \t\t湯姆 是 個 學生 。)\n",
        "\n",
        "- TODO\n",
        "    - Train a simple RNN seq2seq to acheive translation\n",
        "    - Switch to transformer model to boost performance\n",
        "    - Apply Back-translation to furthur boost performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3Vf1Q79XPQ3D"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Apr 14 12:34:02 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 528.49       Driver Version: 528.49       CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
            "|  0%   38C    P8    24W / 275W |    699MiB / 11264MiB |     19%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1744    C+G   ...3d8bbwe\\CalculatorApp.exe    N/A      |\n",
            "|    0   N/A  N/A      6720    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
            "|    0   N/A  N/A      8580    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
            "|    0   N/A  N/A      9512    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
            "|    0   N/A  N/A      9768    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
            "|    0   N/A  N/A     10252    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
            "|    0   N/A  N/A     10476    C+G   ...e\\PhoneExperienceHost.exe    N/A      |\n",
            "|    0   N/A  N/A     10876    C+G   ...oft OneDrive\\OneDrive.exe    N/A      |\n",
            "|    0   N/A  N/A     11536    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
            "|    0   N/A  N/A     11736    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
            "|    0   N/A  N/A     12864    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n",
            "|    0   N/A  N/A     12908    C+G   ...Battle.net\\Battle.net.exe    N/A      |\n",
            "|    0   N/A  N/A     13380    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
            "|    0   N/A  N/A     14332    C+G   ...ge\\Application\\msedge.exe    N/A      |\n",
            "|    0   N/A  N/A     15156    C+G   ...\\app-1.0.9012\\Discord.exe    N/A      |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59neB_Sxp5Ub"
      },
      "source": [
        "# Download and import required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rRlFbfFRpZYT"
      },
      "outputs": [],
      "source": [
        "# !pip install 'torch>=1.6.0' editdistance matplotlib sacrebleu sacremoses sentencepiece tqdm wandb\n",
        "# !pip install --upgrade jupyter ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fSksMTdmp-Wt"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/pytorch/fairseq.git\n",
        "# !cd fairseq && git checkout 9a1c497\n",
        "# !pip install --upgrade ./fairseq/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uRLTiuIuqGNc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\william\\anaconda3\\envs\\hw5-2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import pdb\n",
        "import pprint\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "import numpy as np\n",
        "import tqdm.auto as tqdm\n",
        "from pathlib import Path\n",
        "from argparse import Namespace\n",
        "from fairseq import utils\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'1.23.5'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.version.version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n07Za1XqJzA"
      },
      "source": [
        "# Fix random seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xllxxyWxqI7s"
      },
      "outputs": [],
      "source": [
        "seed = 33\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  \n",
        "np.random.seed(seed)  \n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5ORDJ-2qdYw"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "## En-Zh Bilingual Parallel Corpus\n",
        "* TED2020\n",
        "    - Raw: 400,726 (sentences)   \n",
        "    - Processed: 394,052 (sentences)\n",
        "    \n",
        "\n",
        "## Testdata\n",
        "- Size: 4,000 (sentences)\n",
        "- **Chinese translation is undisclosed. The provided (.zh) file is psuedo translation, each line is a '。'**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQw2mY4Dqkzd"
      },
      "source": [
        "## Dataset Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SXT42xQtqijD"
      },
      "outputs": [],
      "source": [
        "data_dir = './DATA/rawdata'\n",
        "dataset_name = 'ted2020'\n",
        "urls = (\n",
        "    \"https://github.com/figisiwirf/ml2023-hw5-dataset/releases/download/v1.0.1/ml2023.hw5.data.tgz\",\n",
        "    \"https://github.com/figisiwirf/ml2023-hw5-dataset/releases/download/v1.0.1/ml2023.hw5.test.tgz\"\n",
        ")\n",
        "file_names = (\n",
        "    'ted2020.tgz', # train & dev\n",
        "    'test.tgz', # test\n",
        ")\n",
        "prefix = Path(data_dir).absolute() / dataset_name\n",
        "\n",
        "# prefix.mkdir(parents=True, exist_ok=True)\n",
        "# for u, f in zip(urls, file_names):\n",
        "#     path = prefix/f\n",
        "#     if not path.exists():\n",
        "#         !wget {u} -O {path}\n",
        "#     if path.suffix == \".tgz\":\n",
        "#         !tar -xvf {path} -C {prefix}\n",
        "#     elif path.suffix == \".zip\":\n",
        "#         !unzip -o {path} -d {prefix}\n",
        "!mv {prefix/'raw.en'} {prefix/'train_dev.raw.en'}\n",
        "!mv {prefix/'raw.zh'} {prefix/'train_dev.raw.zh'}\n",
        "!mv {prefix/'test.en'} {prefix/'test.raw.en'}\n",
        "!mv {prefix/'test.zh'} {prefix/'test.raw.zh'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLkJwNiFrIwZ"
      },
      "source": [
        "## Language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_uJYkCncrKJb"
      },
      "outputs": [],
      "source": [
        "src_lang = 'en'\n",
        "tgt_lang = 'zh'\n",
        "\n",
        "data_prefix = f'{prefix}/train_dev.raw'\n",
        "test_prefix = f'{prefix}/test.raw'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0t2CPt1brOT3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thank you so much, Chris.\n",
            "And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful.\n",
            "I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night.\n",
            "And I say that sincerely, partly because I need that.\n",
            "Put yourselves in my position.\n",
            "非常謝謝你，克里斯。能有這個機會第二度踏上這個演講台\n",
            "真是一大榮幸。我非常感激。\n",
            "這個研討會給我留下了極為深刻的印象，我想感謝大家 對我之前演講的好評。\n",
            "我是由衷的想這麼說，有部份原因是因為 —— 我真的有需要!\n",
            "請你們設身處地為我想一想！\n"
          ]
        }
      ],
      "source": [
        "!head {data_prefix+'.'+src_lang} -n 5\n",
        "!head {data_prefix+'.'+tgt_lang} -n 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRoE9UK7r1gY"
      },
      "source": [
        "## Preprocess files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3tzFwtnFrle3"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def strQ2B(ustring):\n",
        "    \"\"\"Full width -> half width\"\"\"\n",
        "    # reference:https://ithelp.ithome.com.tw/articles/10233122\n",
        "    ss = []\n",
        "    for s in ustring:\n",
        "        rstring = \"\"\n",
        "        for uchar in s:\n",
        "            inside_code = ord(uchar)\n",
        "            if inside_code == 12288:  # Full width space: direct conversion\n",
        "                inside_code = 32\n",
        "            elif (inside_code >= 65281 and inside_code <= 65374):  # Full width chars (except space) conversion\n",
        "                inside_code -= 65248\n",
        "            rstring += chr(inside_code)\n",
        "        ss.append(rstring)\n",
        "    return ''.join(ss)\n",
        "                \n",
        "def clean_s(s, lang):\n",
        "    if lang == 'en':\n",
        "        s = re.sub(r\"\\([^()]*\\)\", \"\", s) # remove ([text])\n",
        "        s = s.replace('-', '') # remove '-'\n",
        "        s = re.sub('([.,;!?()\\\"])', r' \\1 ', s) # keep punctuation\n",
        "    elif lang == 'zh':\n",
        "        s = strQ2B(s) # Q2B\n",
        "        s = re.sub(r\"\\([^()]*\\)\", \"\", s) # remove ([text])\n",
        "        s = s.replace(' ', '')\n",
        "        s = s.replace('—', '')\n",
        "        s = s.replace('“', '\"')\n",
        "        s = s.replace('”', '\"')\n",
        "        s = s.replace('_', '')\n",
        "        s = re.sub('([。,;!?()\\\"~「」])', r' \\1 ', s) # keep punctuation\n",
        "    s = ' '.join(s.strip().split())\n",
        "    return s\n",
        "\n",
        "def len_s(s, lang):\n",
        "    if lang == 'zh':\n",
        "        return len(s)\n",
        "    return len(s.split())\n",
        "\n",
        "def clean_corpus(prefix, l1, l2, ratio=9, max_len=1000, min_len=1):\n",
        "    if Path(f'{prefix}.clean.{l1}').exists() and Path(f'{prefix}.clean.{l2}').exists():\n",
        "        print(f'{prefix}.clean.{l1} & {l2} exists. skipping clean.')\n",
        "        return\n",
        "    with open(f'{prefix}.{l1}', 'r') as l1_in_f:\n",
        "        with open(f'{prefix}.{l2}', 'r') as l2_in_f:\n",
        "            with open(f'{prefix}.clean.{l1}', 'w') as l1_out_f:\n",
        "                with open(f'{prefix}.clean.{l2}', 'w') as l2_out_f:\n",
        "                    for s1 in l1_in_f:\n",
        "                        s1 = s1.strip()\n",
        "                        s2 = l2_in_f.readline().strip()\n",
        "                        s1 = clean_s(s1, l1)\n",
        "                        s2 = clean_s(s2, l2)\n",
        "                        s1_len = len_s(s1, l1)\n",
        "                        s2_len = len_s(s2, l2)\n",
        "                        if min_len > 0: # remove short sentence\n",
        "                            if s1_len < min_len or s2_len < min_len:\n",
        "                                continue\n",
        "                        if max_len > 0: # remove long sentence\n",
        "                            if s1_len > max_len or s2_len > max_len:\n",
        "                                continue\n",
        "                        if ratio > 0: # remove by ratio of length\n",
        "                            if s1_len/s2_len > ratio or s2_len/s1_len > ratio:\n",
        "                                continue\n",
        "                        print(s1, file=l1_out_f)\n",
        "                        print(s2, file=l2_out_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "h_i8b1PRr9Nf"
      },
      "outputs": [],
      "source": [
        "clean_corpus(data_prefix, src_lang, tgt_lang)\n",
        "clean_corpus(test_prefix, src_lang, tgt_lang, ratio=-1, min_len=-1, max_len=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gjT3XCy9r_rj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thank you so much , Chris .\n",
            "And it's truly a great honor to have the opportunity to come to this stage twice ; I'm extremely grateful .\n",
            "I have been blown away by this conference , and I want to thank all of you for the many nice comments about what I had to say the other night .\n",
            "And I say that sincerely , partly because I need that .\n",
            "Put yourselves in my position .\n",
            "非常謝謝你 , 克里斯 。 能有這個機會第二度踏上這個演講台\n",
            "真是一大榮幸 。 我非常感激 。\n",
            "這個研討會給我留下了極為深刻的印象 , 我想感謝大家對我之前演講的好評 。\n",
            "我是由衷的想這麼說 , 有部份原因是因為我真的有需要 !\n",
            "請你們設身處地為我想一想 !\n"
          ]
        }
      ],
      "source": [
        "!head {data_prefix+'.clean.'+src_lang} -n 5\n",
        "!head {data_prefix+'.clean.'+tgt_lang} -n 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKb4u67-sT_Z"
      },
      "source": [
        "## Split into train/valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "AuFKeDz3sGHL"
      },
      "outputs": [],
      "source": [
        "valid_ratio = 0.01 # 3000~4000 would suffice\n",
        "train_ratio = 1 - valid_ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "QR2NVldqsXyY"
      },
      "outputs": [],
      "source": [
        "if (prefix/f'train.clean.{src_lang}').exists() \\\n",
        "and (prefix/f'train.clean.{tgt_lang}').exists() \\\n",
        "and (prefix/f'valid.clean.{src_lang}').exists() \\\n",
        "and (prefix/f'valid.clean.{tgt_lang}').exists():\n",
        "    print(f'train/valid splits exists. skipping split.')\n",
        "else:\n",
        "    line_num = sum(1 for line in open(f'{data_prefix}.clean.{src_lang}'))\n",
        "    labels = list(range(line_num))\n",
        "    random.shuffle(labels)\n",
        "    for lang in [src_lang, tgt_lang]:\n",
        "        train_f = open(os.path.join(data_dir, dataset_name, f'train.clean.{lang}'), 'w')\n",
        "        valid_f = open(os.path.join(data_dir, dataset_name, f'valid.clean.{lang}'), 'w')\n",
        "        count = 0\n",
        "        for line in open(f'{data_prefix}.clean.{lang}', 'r'):\n",
        "            if labels[count]/line_num < train_ratio:\n",
        "                train_f.write(line)\n",
        "            else:\n",
        "                valid_f.write(line)\n",
        "            count += 1\n",
        "        train_f.close()\n",
        "        valid_f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1rwQysTsdJq"
      },
      "source": [
        "## Subword Units \n",
        "Out of vocabulary (OOV) has been a major problem in machine translation. This can be alleviated by using subword units.\n",
        "- We will use the [sentencepiece](#kudo-richardson-2018-sentencepiece) package\n",
        "- select 'unigram' or 'byte-pair encoding (BPE)' algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Ecwllsa7sZRA"
      },
      "outputs": [],
      "source": [
        "import sentencepiece as spm\n",
        "vocab_size = 8000\n",
        "if (prefix/f'spm{vocab_size}.model').exists():\n",
        "    print(f'{prefix}/spm{vocab_size}.model exists. skipping spm_train.')\n",
        "else:\n",
        "    spm.SentencePieceTrainer.train(\n",
        "        input=','.join([f'{prefix}/train.clean.{src_lang}',\n",
        "                        f'{prefix}/valid.clean.{src_lang}',\n",
        "                        f'{prefix}/train.clean.{tgt_lang}',\n",
        "                        f'{prefix}/valid.clean.{tgt_lang}']),\n",
        "        model_prefix=prefix/f'spm{vocab_size}',\n",
        "        vocab_size=vocab_size,\n",
        "        character_coverage=1,\n",
        "        model_type='unigram', # 'bpe' works as well\n",
        "        input_sentence_size=1e6,\n",
        "        shuffle_input_sentence=True,\n",
        "        normalization_rule_name='nmt_nfkc_cf',\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "lQPRNldqse_V"
      },
      "outputs": [],
      "source": [
        "spm_model = spm.SentencePieceProcessor(model_file=str(prefix/f'spm{vocab_size}.model'))\n",
        "in_tag = {\n",
        "    'train': 'train.clean',\n",
        "    'valid': 'valid.clean',\n",
        "    'test': 'test.raw.clean',\n",
        "}\n",
        "for split in ['train', 'valid', 'test']:\n",
        "    for lang in [src_lang, tgt_lang]:\n",
        "        out_path = prefix/f'{split}.{lang}'\n",
        "        if out_path.exists():\n",
        "            print(f\"{out_path} exists. skipping spm_encode.\")\n",
        "        else:\n",
        "            with open(prefix/f'{split}.{lang}', 'w') as out_f:\n",
        "                with open(prefix/f'{in_tag[split]}.{lang}', 'r') as in_f:\n",
        "                    for line in in_f:\n",
        "                        line = line.strip()\n",
        "                        tok = spm_model.encode(line, out_type=str)\n",
        "                        print(' '.join(tok), file=out_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "4j6lXHjAsjXa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▁thank ▁you ▁so ▁much ▁, ▁chris ▁.\n",
            "▁and ▁it ' s ▁tr u ly ▁a ▁great ▁ho n or ▁to ▁have ▁the ▁ op port un ity ▁to ▁come ▁to ▁this ▁st age ▁ t wi ce ▁ ; ▁i ' m ▁ex t re me ly ▁gr ate ful ▁.\n",
            "▁i ▁have ▁been ▁ bl own ▁away ▁by ▁this ▁con fer ence ▁, ▁and ▁i ▁want ▁to ▁thank ▁all ▁of ▁you ▁for ▁the ▁many ▁ ni ce ▁ com ment s ▁about ▁what ▁i ▁had ▁to ▁say ▁the ▁other ▁night ▁.\n",
            "▁and ▁i ▁say ▁that ▁since re ly ▁, ▁part ly ▁because ▁i ▁need ▁that ▁.\n",
            "▁put ▁your s el ve s ▁in ▁my ▁ position ▁.\n",
            "▁ 非常 謝 謝 你 ▁, ▁ 克 里 斯 ▁。 ▁ 能 有 這個 機會 第二 度 踏 上 這個 演講 台\n",
            "▁ 真 是 一 大 榮 幸 ▁。 ▁我 非常 感 激 ▁。\n",
            "▁這個 研 討 會 給我 留 下 了 極 為 深 刻 的 印 象 ▁, ▁我想 感 謝 大家 對我 之前 演講 的 好 評 ▁。\n",
            "▁我 是由 衷 的 想 這麼 說 ▁, ▁有 部份 原因 是因為 我 真的 有 需要 ▁ !\n",
            "▁ 請 你們 設 身 處 地 為 我想 一 想 ▁ !\n"
          ]
        }
      ],
      "source": [
        "!head {data_dir+'/'+dataset_name+'/train.'+src_lang} -n 5\n",
        "!head {data_dir+'/'+dataset_name+'/train.'+tgt_lang} -n 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59si_C0Wsms7"
      },
      "source": [
        "## Binarize the data with fairseq\n",
        "Prepare the files in pairs for both the source and target languages. \\\\\n",
        "In case a pair is unavailable, generate a pseudo pair to facilitate binarization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "w-cHVLSpsknh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 12:35:49 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='DATA\\\\data-bin\\\\ted2020', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=True, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='en', srcdict=None, suppress_crashes=False, target_lang='zh', task='translation', tensorboard_logdir=None, testpref='c:\\\\Users\\\\william\\\\Desktop\\\\graddescope\\\\DATA\\\\rawdata\\\\ted2020\\\\test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='c:\\\\Users\\\\william\\\\Desktop\\\\graddescope\\\\DATA\\\\rawdata\\\\ted2020\\\\train', user_dir=None, validpref='c:\\\\Users\\\\william\\\\Desktop\\\\graddescope\\\\DATA\\\\rawdata\\\\ted2020\\\\valid', wandb_project=None, workers=2)\n",
            "2023-04-14 12:36:15 | INFO | fairseq_cli.preprocess | [en] Dictionary: 7992 types\n",
            "2023-04-14 12:36:42 | INFO | fairseq_cli.preprocess | [en] c:\\Users\\william\\Desktop\\graddescope\\DATA\\rawdata\\ted2020\\train.en: 390112 sents, 12327245 tokens, 0.0% replaced by <unk>\n",
            "2023-04-14 12:36:42 | INFO | fairseq_cli.preprocess | [en] Dictionary: 7992 types\n",
            "2023-04-14 12:36:45 | INFO | fairseq_cli.preprocess | [en] c:\\Users\\william\\Desktop\\graddescope\\DATA\\rawdata\\ted2020\\valid.en: 3940 sents, 123018 tokens, 0.0% replaced by <unk>\n",
            "2023-04-14 12:36:45 | INFO | fairseq_cli.preprocess | [en] Dictionary: 7992 types\n",
            "2023-04-14 12:36:47 | INFO | fairseq_cli.preprocess | [en] c:\\Users\\william\\Desktop\\graddescope\\DATA\\rawdata\\ted2020\\test.en: 4000 sents, 128045 tokens, 0.0% replaced by <unk>\n",
            "2023-04-14 12:36:47 | INFO | fairseq_cli.preprocess | [zh] Dictionary: 7992 types\n",
            "2023-04-14 12:37:11 | INFO | fairseq_cli.preprocess | [zh] c:\\Users\\william\\Desktop\\graddescope\\DATA\\rawdata\\ted2020\\train.zh: 390112 sents, 9599514 tokens, 0.0% replaced by <unk>\n",
            "2023-04-14 12:37:11 | INFO | fairseq_cli.preprocess | [zh] Dictionary: 7992 types\n",
            "2023-04-14 12:37:13 | INFO | fairseq_cli.preprocess | [zh] c:\\Users\\william\\Desktop\\graddescope\\DATA\\rawdata\\ted2020\\valid.zh: 3940 sents, 95760 tokens, 0.0146% replaced by <unk>\n",
            "2023-04-14 12:37:13 | INFO | fairseq_cli.preprocess | [zh] Dictionary: 7992 types\n",
            "2023-04-14 12:37:15 | INFO | fairseq_cli.preprocess | [zh] c:\\Users\\william\\Desktop\\graddescope\\DATA\\rawdata\\ted2020\\test.zh: 4000 sents, 8000 tokens, 0.0% replaced by <unk>\n",
            "2023-04-14 12:37:15 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to DATA\\data-bin\\ted2020\n"
          ]
        }
      ],
      "source": [
        "binpath = Path('./DATA/data-bin', dataset_name)\n",
        "if binpath.exists():\n",
        "    print(binpath, \"exists, will not overwrite!\")\n",
        "else:\n",
        "    !python -m fairseq_cli.preprocess \\\n",
        "        --source-lang {src_lang}\\\n",
        "        --target-lang {tgt_lang}\\\n",
        "        --trainpref {prefix/'train'}\\\n",
        "        --validpref {prefix/'valid'}\\\n",
        "        --testpref {prefix/'test'}\\\n",
        "        --destdir {binpath}\\\n",
        "        --joined-dictionary\\\n",
        "        --workers 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szMuH1SWLPWA"
      },
      "source": [
        "# Configuration for experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "5Luz3_tVLUxs"
      },
      "outputs": [],
      "source": [
        "config = Namespace(\n",
        "    datadir = \"./DATA/data-bin/ted2020\",\n",
        "    # savedir = \"./checkpoints/rnn\",\n",
        "    savedir = \"./checkpoints/transformer\",\n",
        "    source_lang = src_lang,\n",
        "    target_lang = tgt_lang,\n",
        "    \n",
        "    # cpu threads when fetching & processing data.\n",
        "    num_workers=2,  \n",
        "    # batch size in terms of tokens. gradient accumulation increases the effective batchsize.\n",
        "    max_tokens=8192,\n",
        "    accum_steps=2,\n",
        "    \n",
        "    # the lr s calculated from Noam lr scheduler. you can tune the maximum lr by this factor.\n",
        "    lr_factor=2.,\n",
        "    lr_warmup=4000,\n",
        "    \n",
        "    # clipping gradient norm helps alleviate gradient exploding\n",
        "    clip_norm=1.0,\n",
        "    \n",
        "    # maximum epochs for training\n",
        "    max_epoch=60,\n",
        "    start_epoch=1,\n",
        "    \n",
        "    # beam size for beam search\n",
        "    beam=5, \n",
        "    # generate sequences of maximum length ax + b, where x is the source length\n",
        "    max_len_a=1.2, \n",
        "    max_len_b=10, \n",
        "    # when decoding, post process sentence by removing sentencepiece symbols and jieba tokenization.\n",
        "    post_process = \"sentencepiece\",\n",
        "    \n",
        "    # checkpoints\n",
        "    keep_last_epochs=5,\n",
        "    resume=None, # if resume from checkpoint name (under config.savedir)\n",
        "    \n",
        "    # logging\n",
        "    use_wandb=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjrJFvyQLg86"
      },
      "source": [
        "# Logging\n",
        "- logging package logs ordinary messages\n",
        "- wandb logs the loss, bleu, etc. in the training process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "-ZiMyDWALbDk"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "    level=\"INFO\", # \"DEBUG\" \"WARNING\" \"ERROR\"\n",
        "    stream=sys.stdout,\n",
        ")\n",
        "proj = \"hw5.seq2seq\"\n",
        "logger = logging.getLogger(proj)\n",
        "if config.use_wandb:\n",
        "    import wandb\n",
        "    wandb.init(project=proj, name=Path(config.savedir).stem, config=config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNoSkK45Lmqc"
      },
      "source": [
        "# CUDA Environments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "oqrsbmcoLqMl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 12:37:16 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-04-14 12:37:16 | INFO | fairseq.utils | rank   0: capabilities =  6.1  ; total memory = 11.000 GB ; name = NVIDIA GeForce GTX 1080 Ti              \n",
            "2023-04-14 12:37:16 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n"
          ]
        }
      ],
      "source": [
        "cuda_env = utils.CudaEnvironment()\n",
        "utils.CudaEnvironment.pretty_print_cuda_env_list([cuda_env])\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbJuBIHLLt2D"
      },
      "source": [
        "# Dataloading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOpG4EBRLwe_"
      },
      "source": [
        "## We borrow the TranslationTask from fairseq\n",
        "* used to load the binarized data created above\n",
        "* well-implemented data iterator (dataloader)\n",
        "* built-in task.source_dictionary and task.target_dictionary are also handy\n",
        "* well-implemented beach search decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "3gSEy1uFLvVs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 12:37:16 | INFO | fairseq.tasks.translation | [en] dictionary: 7992 types\n",
            "2023-04-14 12:37:16 | INFO | fairseq.tasks.translation | [zh] dictionary: 7992 types\n"
          ]
        }
      ],
      "source": [
        "from fairseq.tasks.translation import TranslationConfig, TranslationTask\n",
        "\n",
        "## setup task\n",
        "task_cfg = TranslationConfig(\n",
        "    data=config.datadir,\n",
        "    source_lang=config.source_lang,\n",
        "    target_lang=config.target_lang,\n",
        "    train_subset=\"train\",\n",
        "    required_seq_len_multiple=8,\n",
        "    dataset_impl=\"mmap\",\n",
        "    upsample_primary=1,\n",
        ")\n",
        "task = TranslationTask.setup_task(task_cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "mR7Bhov7L4IU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 12:37:16 | INFO | hw5.seq2seq | loading data for epoch 1\n",
            "2023-04-14 12:37:16 | INFO | fairseq.data.data_utils | loaded 390,112 examples from: ./DATA/data-bin/ted2020\\train.en-zh.en\n",
            "2023-04-14 12:37:16 | INFO | fairseq.data.data_utils | loaded 390,112 examples from: ./DATA/data-bin/ted2020\\train.en-zh.zh\n",
            "2023-04-14 12:37:16 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020 train en-zh 390112 examples\n",
            "2023-04-14 12:37:16 | INFO | fairseq.data.data_utils | loaded 3,940 examples from: ./DATA/data-bin/ted2020\\valid.en-zh.en\n",
            "2023-04-14 12:37:16 | INFO | fairseq.data.data_utils | loaded 3,940 examples from: ./DATA/data-bin/ted2020\\valid.en-zh.zh\n",
            "2023-04-14 12:37:16 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020 valid en-zh 3940 examples\n"
          ]
        }
      ],
      "source": [
        "logger.info(\"loading data for epoch 1\")\n",
        "task.load_dataset(split=\"train\", epoch=1, combine=True) # combine if you have back-translation data.\n",
        "task.load_dataset(split=\"valid\", epoch=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "P0BCEm_9L6ig"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 1,\n",
            " 'source': tensor([  24,   64,    5,   85, 1299,  142,  144,  190,  274,   37,    8,   88,\n",
            "         237,   11,   78,   55,   12,  372,   20,  154,   62, 1012,  100,  484,\n",
            "          75,  268,    6,  100, 1463,    7,    2]),\n",
            " 'target': tensor([ 162,  125, 3759,  359,  157, 3058, 2923,    9, 2550,    4,  598,  123,\n",
            "        1515,  551,  664,   65,  406,  570,   77, 1907, 3793,  189,   10,    2])}\n",
            "('Source: you can throw out crazy theories and not have to back it up with '\n",
            " 'data or graphs or research .')\n",
            "'Target: 你能拋開這些瘋狂的理論 , 不用數據圖表、或研究來支撐它 。'\n"
          ]
        }
      ],
      "source": [
        "sample = task.dataset(\"valid\")[1]\n",
        "pprint.pprint(sample)\n",
        "pprint.pprint(\n",
        "    \"Source: \" + \\\n",
        "    task.source_dictionary.string(\n",
        "        sample['source'],\n",
        "        config.post_process,\n",
        "    )\n",
        ")\n",
        "pprint.pprint(\n",
        "    \"Target: \" + \\\n",
        "    task.target_dictionary.string(\n",
        "        sample['target'],\n",
        "        config.post_process,\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcfCVa2FMBSE"
      },
      "source": [
        "# Dataset iterator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBvc-B_6MKZM"
      },
      "source": [
        "* Controls every batch to contain no more than N tokens, which optimizes GPU memory efficiency\n",
        "* Shuffles the training set for every epoch\n",
        "* Ignore sentences exceeding maximum length\n",
        "* Pad all sentences in a batch to the same length, which enables parallel computing by GPU\n",
        "* Add eos and shift one token\n",
        "    - teacher forcing: to train the model to predict the next token based on prefix, we feed the right shifted target sequence as the decoder input.\n",
        "    - generally, prepending bos to the target would do the job (as shown below)\n",
        "![seq2seq](https://i.imgur.com/0zeDyuI.png)\n",
        "    - in fairseq however, this is done by moving the eos token to the begining. Empirically, this has the same effect. For instance:\n",
        "    ```\n",
        "    # output target (target) and Decoder input (prev_output_tokens): \n",
        "                   eos = 2\n",
        "                target = 419,  711,  238,  888,  792,   60,  968,    8,    2\n",
        "    prev_output_tokens = 2,  419,  711,  238,  888,  792,   60,  968,    8\n",
        "    ```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "OWFJFmCnMDXW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 12:37:16 | WARNING | fairseq.tasks.fairseq_task | 2,526 samples have invalid sizes and will be skipped, max_positions=(20, 20), first few sample ids=[1856, 1791, 2936, 918, 3540, 3373, 1246, 3167, 3558, 3250]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'id': tensor([3381]),\n",
              " 'nsentences': 1,\n",
              " 'ntokens': 12,\n",
              " 'net_input': {'src_tokens': tensor([[  11,  259,  289,    4,   15, 1088,   19,   14,   36,  230,    4,  259,\n",
              "           1600,  122,    7,    2]]),\n",
              "  'src_lengths': tensor([16]),\n",
              "  'prev_output_tokens': tensor([[   2, 1607,    4, 2475, 1799,    4,  242,  467, 2208, 2831,   34,   10,\n",
              "              1,    1,    1,    1]])},\n",
              " 'target': tensor([[1607,    4, 2475, 1799,    4,  242,  467, 2208, 2831,   34,   10,    2,\n",
              "             1,    1,    1,    1]])}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def load_data_iterator(task, split, epoch=1, max_tokens=4000, num_workers=1, cached=True):\n",
        "    batch_iterator = task.get_batch_iterator(\n",
        "        dataset=task.dataset(split),\n",
        "        max_tokens=max_tokens,\n",
        "        max_sentences=None,\n",
        "        max_positions=utils.resolve_max_positions(\n",
        "            task.max_positions(),\n",
        "            max_tokens,\n",
        "        ),\n",
        "        ignore_invalid_inputs=True,\n",
        "        seed=seed,\n",
        "        num_workers=num_workers,\n",
        "        epoch=epoch,\n",
        "        disable_iterator_cache=not cached,\n",
        "        # Set this to False to speed up. However, if set to False, changing max_tokens beyond \n",
        "        # first call of this method has no effect. \n",
        "    )\n",
        "    return batch_iterator\n",
        "\n",
        "demo_epoch_obj = load_data_iterator(task, \"valid\", epoch=1, max_tokens=20, num_workers=1, cached=False)\n",
        "demo_iter = demo_epoch_obj.next_epoch_itr(shuffle=True)\n",
        "sample = next(demo_iter)\n",
        "sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p86K-0g7Me4M"
      },
      "source": [
        "* each batch is a python dict, with string key and Tensor value. Contents are described below:\n",
        "```python\n",
        "batch = {\n",
        "    \"id\": id, # id for each example \n",
        "    \"nsentences\": len(samples), # batch size (sentences)\n",
        "    \"ntokens\": ntokens, # batch size (tokens)\n",
        "    \"net_input\": {\n",
        "        \"src_tokens\": src_tokens, # sequence in source language\n",
        "        \"src_lengths\": src_lengths, # sequence length of each example before padding\n",
        "        \"prev_output_tokens\": prev_output_tokens, # right shifted target, as mentioned above.\n",
        "    },\n",
        "    \"target\": target, # target sequence\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EyDBE5ZMkFZ"
      },
      "source": [
        "# Model Architecture\n",
        "* We again inherit fairseq's encoder, decoder and model, so that in the testing phase we can directly leverage fairseq's beam search decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Hzh74qLIMfW_"
      },
      "outputs": [],
      "source": [
        "from fairseq.models import (\n",
        "    FairseqEncoder, \n",
        "    FairseqIncrementalDecoder,\n",
        "    FairseqEncoderDecoderModel\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI46v1z7MotH"
      },
      "source": [
        "# Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn0wSeLLMrbc"
      },
      "source": [
        "- The Encoder is a RNN or Transformer Encoder. The following description is for RNN. For every input token, Encoder will generate a output vector and a hidden states vector, and the hidden states vector is passed on to the next step. In other words, the Encoder sequentially reads in the input sequence, and outputs a single vector at each timestep, then finally outputs the final hidden states, or content vector, at the last timestep.\n",
        "- Parameters:\n",
        "  - *args*\n",
        "      - encoder_embed_dim: the dimension of embeddings, this compresses the one-hot vector into fixed dimensions, which achieves dimension reduction\n",
        "      - encoder_ffn_embed_dim is the dimension of hidden states and output vectors\n",
        "      - encoder_layers is the number of layers for Encoder RNN\n",
        "      - dropout determines the probability of a neuron's activation being set to 0, in order to prevent overfitting. Generally this is applied in training, and removed in testing.\n",
        "  - *dictionary*: the dictionary provided by fairseq. it's used to obtain the padding index, and in turn the encoder padding mask. \n",
        "  - *embed_tokens*: an instance of token embeddings (nn.Embedding)\n",
        "\n",
        "- Inputs: \n",
        "    - *src_tokens*: integer sequence representing english e.g. 1, 28, 29, 205, 2 \n",
        "- Outputs: \n",
        "    - *outputs*: the output of RNN at each timestep, can be furthur processed by Attention\n",
        "    - *final_hiddens*: the hidden states of each timestep, will be passed to decoder for decoding\n",
        "    - *encoder_padding_mask*: this tells the decoder which position to ignore\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "WcX3W4iGMq-S"
      },
      "outputs": [],
      "source": [
        "class RNNEncoder(FairseqEncoder):\n",
        "    def __init__(self, args, dictionary, embed_tokens):\n",
        "        super().__init__(dictionary)\n",
        "        self.embed_tokens = embed_tokens\n",
        "        \n",
        "        self.embed_dim = args.encoder_embed_dim\n",
        "        self.hidden_dim = args.encoder_ffn_embed_dim\n",
        "        self.num_layers = args.encoder_layers\n",
        "        \n",
        "        self.dropout_in_module = nn.Dropout(args.dropout)\n",
        "        self.rnn = nn.GRU(\n",
        "            self.embed_dim, \n",
        "            self.hidden_dim, \n",
        "            self.num_layers, \n",
        "            dropout=args.dropout, \n",
        "            batch_first=False, \n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.dropout_out_module = nn.Dropout(args.dropout)\n",
        "        \n",
        "        self.padding_idx = dictionary.pad()\n",
        "        \n",
        "    def combine_bidir(self, outs, bsz: int):\n",
        "        out = outs.view(self.num_layers, 2, bsz, -1).transpose(1, 2).contiguous()\n",
        "        return out.view(self.num_layers, bsz, -1)\n",
        "\n",
        "    def forward(self, src_tokens, **unused):\n",
        "        bsz, seqlen = src_tokens.size()\n",
        "        \n",
        "        # get embeddings\n",
        "        x = self.embed_tokens(src_tokens)\n",
        "        x = self.dropout_in_module(x)\n",
        "\n",
        "        # B x T x C -> T x B x C\n",
        "        x = x.transpose(0, 1)\n",
        "        \n",
        "        # pass thru bidirectional RNN\n",
        "        h0 = x.new_zeros(2 * self.num_layers, bsz, self.hidden_dim)\n",
        "        x, final_hiddens = self.rnn(x, h0)\n",
        "        outputs = self.dropout_out_module(x)\n",
        "        # outputs = [sequence len, batch size, hid dim * directions]\n",
        "        # hidden =  [num_layers * directions, batch size  , hid dim]\n",
        "        \n",
        "        # Since Encoder is bidirectional, we need to concatenate the hidden states of two directions\n",
        "        final_hiddens = self.combine_bidir(final_hiddens, bsz)\n",
        "        # hidden =  [num_layers x batch x num_directions*hidden]\n",
        "        \n",
        "        encoder_padding_mask = src_tokens.eq(self.padding_idx).t()\n",
        "        return tuple(\n",
        "            (\n",
        "                outputs,  # seq_len x batch x hidden\n",
        "                final_hiddens,  # num_layers x batch x num_directions*hidden\n",
        "                encoder_padding_mask,  # seq_len x batch\n",
        "            )\n",
        "        )\n",
        "    \n",
        "    def reorder_encoder_out(self, encoder_out, new_order):\n",
        "        # This is used by fairseq's beam search. How and why is not particularly important here.\n",
        "        return tuple(\n",
        "            (\n",
        "                encoder_out[0].index_select(1, new_order),\n",
        "                encoder_out[1].index_select(1, new_order),\n",
        "                encoder_out[2].index_select(1, new_order),\n",
        "            )\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZlE_1JnMv56"
      },
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSFSKt_ZMzgh"
      },
      "source": [
        "- When the input sequence is long, \"content vector\" alone cannot accurately represent the whole sequence, attention mechanism can provide the Decoder more information.\n",
        "- According to the **Decoder embeddings** of the current timestep, match the **Encoder outputs** with decoder embeddings to determine correlation, and then sum the Encoder outputs weighted by the correlation as the input to **Decoder** RNN.\n",
        "- Common attention implementations use neural network / dot product as the correlation between **query** (decoder embeddings) and **key** (Encoder outputs), followed by **softmax**  to obtain a distribution, and finally **values** (Encoder outputs) is **weighted sum**-ed by said distribution.\n",
        "\n",
        "- Parameters:\n",
        "  - *input_embed_dim*: dimensionality of key, should be that of the vector in decoder to attend others\n",
        "  - *source_embed_dim*: dimensionality of query, should be that of the vector to be attended to (encoder outputs)\n",
        "  - *output_embed_dim*: dimensionality of value, should be that of the vector after attention, expected by the next layer\n",
        "\n",
        "- Inputs: \n",
        "    - *inputs*: is the key, the vector to attend to others\n",
        "    - *encoder_outputs*:  is the query/value, the vector to be attended to\n",
        "    - *encoder_padding_mask*: this tells the decoder which position to ignore\n",
        "- Outputs: \n",
        "    - *output*: the context vector after attention\n",
        "    - *attention score*: the attention distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "1Atf_YuCMyyF"
      },
      "outputs": [],
      "source": [
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, input_embed_dim, source_embed_dim, output_embed_dim, bias=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_proj = nn.Linear(input_embed_dim, source_embed_dim, bias=bias)\n",
        "        self.output_proj = nn.Linear(\n",
        "            input_embed_dim + source_embed_dim, output_embed_dim, bias=bias\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs, encoder_outputs, encoder_padding_mask):\n",
        "        # inputs: T, B, dim\n",
        "        # encoder_outputs: S x B x dim\n",
        "        # padding mask:  S x B\n",
        "        \n",
        "        # convert all to batch first\n",
        "        inputs = inputs.transpose(1,0) # B, T, dim\n",
        "        encoder_outputs = encoder_outputs.transpose(1,0) # B, S, dim\n",
        "        encoder_padding_mask = encoder_padding_mask.transpose(1,0) # B, S\n",
        "        \n",
        "        # project to the dimensionality of encoder_outputs\n",
        "        x = self.input_proj(inputs)\n",
        "\n",
        "        # compute attention\n",
        "        # (B, T, dim) x (B, dim, S) = (B, T, S)\n",
        "        attn_scores = torch.bmm(x, encoder_outputs.transpose(1,2))\n",
        "\n",
        "        # cancel the attention at positions corresponding to padding\n",
        "        if encoder_padding_mask is not None:\n",
        "            # leveraging broadcast  B, S -> (B, 1, S)\n",
        "            encoder_padding_mask = encoder_padding_mask.unsqueeze(1)\n",
        "            attn_scores = (\n",
        "                attn_scores.float()\n",
        "                .masked_fill_(encoder_padding_mask, float(\"-inf\"))\n",
        "                .type_as(attn_scores)\n",
        "            )  # FP16 support: cast to float and back\n",
        "\n",
        "        # softmax on the dimension corresponding to source sequence\n",
        "        attn_scores = F.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        # shape (B, T, S) x (B, S, dim) = (B, T, dim) weighted sum\n",
        "        x = torch.bmm(attn_scores, encoder_outputs)\n",
        "\n",
        "        # (B, T, dim)\n",
        "        x = torch.cat((x, inputs), dim=-1)\n",
        "        x = torch.tanh(self.output_proj(x)) # concat + linear + tanh\n",
        "        \n",
        "        # restore shape (B, T, dim) -> (T, B, dim)\n",
        "        return x.transpose(1,0), attn_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doSCOA2gM7fK"
      },
      "source": [
        "# Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M8Vod2gNABR"
      },
      "source": [
        "* The hidden states of **Decoder** will be initialized by the final hidden states of **Encoder** (the content vector)\n",
        "* At the same time, **Decoder** will change its hidden states based on the input of the current timestep (the outputs of previous timesteps), and generates an output\n",
        "* Attention improves the performance\n",
        "* The seq2seq steps are implemented in decoder, so that later the Seq2Seq class can accept RNN and Transformer, without furthur modification.\n",
        "- Parameters:\n",
        "  - *args*\n",
        "      - decoder_embed_dim: is the dimensionality of the decoder embeddings, similar to encoder_embed_dim，\n",
        "      - decoder_ffn_embed_dim: is the dimensionality of the decoder RNN hidden states, similar to encoder_ffn_embed_dim\n",
        "      - decoder_layers: number of layers of RNN decoder\n",
        "      - share_decoder_input_output_embed: usually, the projection matrix of the decoder will share weights with the decoder input embeddings\n",
        "  - *dictionary*: the dictionary provided by fairseq\n",
        "  - *embed_tokens*: an instance of token embeddings (nn.Embedding)\n",
        "- Inputs: \n",
        "    - *prev_output_tokens*: integer sequence representing the right-shifted target e.g. 1, 28, 29, 205, 2 \n",
        "    - *encoder_out*: encoder's output.\n",
        "    - *incremental_state*: in order to speed up decoding during test time, we will save the hidden state of each timestep. see forward() for details.\n",
        "- Outputs: \n",
        "    - *outputs*: the logits (before softmax) output of decoder for each timesteps\n",
        "    - *extra*: unsused"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "QfvgqHYDM6Lp"
      },
      "outputs": [],
      "source": [
        "class RNNDecoder(FairseqIncrementalDecoder):\n",
        "    def __init__(self, args, dictionary, embed_tokens):\n",
        "        super().__init__(dictionary)\n",
        "        self.embed_tokens = embed_tokens\n",
        "        \n",
        "        assert args.decoder_layers == args.encoder_layers, f\"\"\"seq2seq rnn requires that encoder \n",
        "        and decoder have same layers of rnn. got: {args.encoder_layers, args.decoder_layers}\"\"\"\n",
        "        assert args.decoder_ffn_embed_dim == args.encoder_ffn_embed_dim*2, f\"\"\"seq2seq-rnn requires \n",
        "        that decoder hidden to be 2*encoder hidden dim. got: {args.decoder_ffn_embed_dim, args.encoder_ffn_embed_dim*2}\"\"\"\n",
        "        \n",
        "        self.embed_dim = args.decoder_embed_dim\n",
        "        self.hidden_dim = args.decoder_ffn_embed_dim\n",
        "        self.num_layers = args.decoder_layers\n",
        "        \n",
        "        \n",
        "        self.dropout_in_module = nn.Dropout(args.dropout)\n",
        "        self.rnn = nn.GRU(\n",
        "            self.embed_dim, \n",
        "            self.hidden_dim, \n",
        "            self.num_layers, \n",
        "            dropout=args.dropout, \n",
        "            batch_first=False, \n",
        "            bidirectional=False\n",
        "        )\n",
        "        self.attention = AttentionLayer(\n",
        "            self.embed_dim, self.hidden_dim, self.embed_dim, bias=False\n",
        "        ) \n",
        "        # self.attention = None\n",
        "        self.dropout_out_module = nn.Dropout(args.dropout)\n",
        "        \n",
        "        if self.hidden_dim != self.embed_dim:\n",
        "            self.project_out_dim = nn.Linear(self.hidden_dim, self.embed_dim)\n",
        "        else:\n",
        "            self.project_out_dim = None\n",
        "        \n",
        "        if args.share_decoder_input_output_embed:\n",
        "            self.output_projection = nn.Linear(\n",
        "                self.embed_tokens.weight.shape[1],\n",
        "                self.embed_tokens.weight.shape[0],\n",
        "                bias=False,\n",
        "            )\n",
        "            self.output_projection.weight = self.embed_tokens.weight\n",
        "        else:\n",
        "            self.output_projection = nn.Linear(\n",
        "                self.output_embed_dim, len(dictionary), bias=False\n",
        "            )\n",
        "            nn.init.normal_(\n",
        "                self.output_projection.weight, mean=0, std=self.output_embed_dim ** -0.5\n",
        "            )\n",
        "        \n",
        "    def forward(self, prev_output_tokens, encoder_out, incremental_state=None, **unused):\n",
        "        # extract the outputs from encoder\n",
        "        encoder_outputs, encoder_hiddens, encoder_padding_mask = encoder_out\n",
        "        # outputs:          seq_len x batch x num_directions*hidden\n",
        "        # encoder_hiddens:  num_layers x batch x num_directions*encoder_hidden\n",
        "        # padding_mask:     seq_len x batch\n",
        "        \n",
        "        if incremental_state is not None and len(incremental_state) > 0:\n",
        "            # if the information from last timestep is retained, we can continue from there instead of starting from bos\n",
        "            prev_output_tokens = prev_output_tokens[:, -1:]\n",
        "            cache_state = self.get_incremental_state(incremental_state, \"cached_state\")\n",
        "            prev_hiddens = cache_state[\"prev_hiddens\"]\n",
        "        else:\n",
        "            # incremental state does not exist, either this is training time, or the first timestep of test time\n",
        "            # prepare for seq2seq: pass the encoder_hidden to the decoder hidden states\n",
        "            prev_hiddens = encoder_hiddens\n",
        "        \n",
        "        bsz, seqlen = prev_output_tokens.size()\n",
        "        \n",
        "        # embed tokens\n",
        "        x = self.embed_tokens(prev_output_tokens)\n",
        "        x = self.dropout_in_module(x)\n",
        "\n",
        "        # B x T x C -> T x B x C\n",
        "        x = x.transpose(0, 1)\n",
        "                \n",
        "        # decoder-to-encoder attention\n",
        "        if self.attention is not None:\n",
        "            x, attn = self.attention(x, encoder_outputs, encoder_padding_mask)\n",
        "                        \n",
        "        # pass thru unidirectional RNN\n",
        "        x, final_hiddens = self.rnn(x, prev_hiddens)\n",
        "        # outputs = [sequence len, batch size, hid dim]\n",
        "        # hidden =  [num_layers * directions, batch size  , hid dim]\n",
        "        x = self.dropout_out_module(x)\n",
        "                \n",
        "        # project to embedding size (if hidden differs from embed size, and share_embedding is True, \n",
        "        # we need to do an extra projection)\n",
        "        if self.project_out_dim != None:\n",
        "            x = self.project_out_dim(x)\n",
        "        \n",
        "        # project to vocab size\n",
        "        x = self.output_projection(x)\n",
        "        \n",
        "        # T x B x C -> B x T x C\n",
        "        x = x.transpose(1, 0)\n",
        "        \n",
        "        # if incremental, record the hidden states of current timestep, which will be restored in the next timestep\n",
        "        cache_state = {\n",
        "            \"prev_hiddens\": final_hiddens,\n",
        "        }\n",
        "        self.set_incremental_state(incremental_state, \"cached_state\", cache_state)\n",
        "        \n",
        "        return x, None\n",
        "    \n",
        "    def reorder_incremental_state(\n",
        "        self,\n",
        "        incremental_state,\n",
        "        new_order,\n",
        "    ):\n",
        "        # This is used by fairseq's beam search. How and why is not particularly important here.\n",
        "        cache_state = self.get_incremental_state(incremental_state, \"cached_state\")\n",
        "        prev_hiddens = cache_state[\"prev_hiddens\"]\n",
        "        prev_hiddens = [p.index_select(0, new_order) for p in prev_hiddens]\n",
        "        cache_state = {\n",
        "            \"prev_hiddens\": torch.stack(prev_hiddens),\n",
        "        }\n",
        "        self.set_incremental_state(incremental_state, \"cached_state\", cache_state)\n",
        "        return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDAPmxjRNEEL"
      },
      "source": [
        "## Seq2Seq\n",
        "- Composed of **Encoder** and **Decoder**\n",
        "- Recieves inputs and pass to **Encoder** \n",
        "- Pass the outputs from **Encoder** to **Decoder**\n",
        "- **Decoder** will decode according to outputs of previous timesteps as well as **Encoder** outputs  \n",
        "- Once done decoding, return the **Decoder** outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "oRwKdLa0NEU6"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(FairseqEncoderDecoderModel):\n",
        "    def __init__(self, args, encoder, decoder):\n",
        "        super().__init__(encoder, decoder)\n",
        "        self.args = args\n",
        "    \n",
        "    def forward(\n",
        "        self,\n",
        "        src_tokens,\n",
        "        src_lengths,\n",
        "        prev_output_tokens,\n",
        "        return_all_hiddens: bool = True,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Run the forward pass for an encoder-decoder model.\n",
        "        \"\"\"\n",
        "        encoder_out = self.encoder(\n",
        "            src_tokens, src_lengths=src_lengths, return_all_hiddens=return_all_hiddens\n",
        "        )\n",
        "        logits, extra = self.decoder(\n",
        "            prev_output_tokens,\n",
        "            encoder_out=encoder_out,\n",
        "            src_lengths=src_lengths,\n",
        "            return_all_hiddens=return_all_hiddens,\n",
        "        )\n",
        "        return logits, extra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu3C2JfqNHzk"
      },
      "source": [
        "# Model Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "nyI9FOx-NJ2m"
      },
      "outputs": [],
      "source": [
        "# # HINT: transformer architecture\n",
        "from fairseq.models.transformer import (\n",
        "    TransformerEncoder, \n",
        "    TransformerDecoder,\n",
        ")\n",
        "\n",
        "def build_model(args, task):\n",
        "    \"\"\" build a model instance based on hyperparameters \"\"\"\n",
        "    src_dict, tgt_dict = task.source_dictionary, task.target_dictionary\n",
        "\n",
        "    # token embeddings\n",
        "    encoder_embed_tokens = nn.Embedding(len(src_dict), args.encoder_embed_dim, src_dict.pad())\n",
        "    decoder_embed_tokens = nn.Embedding(len(tgt_dict), args.decoder_embed_dim, tgt_dict.pad())\n",
        "    \n",
        "    # encoder decoder\n",
        "    # HINT: TODO: switch to TransformerEncoder & TransformerDecoder\n",
        "    # encoder = RNNEncoder(args, src_dict, encoder_embed_tokens)\n",
        "    # decoder = RNNDecoder(args, tgt_dict, decoder_embed_tokens)\n",
        "    encoder = TransformerEncoder(args, src_dict, encoder_embed_tokens)\n",
        "    decoder = TransformerDecoder(args, tgt_dict, decoder_embed_tokens)\n",
        "\n",
        "    # sequence to sequence model\n",
        "    model = Seq2Seq(args, encoder, decoder)\n",
        "    \n",
        "    # initialization for seq2seq model is important, requires extra handling\n",
        "    def init_params(module):\n",
        "        from fairseq.modules import MultiheadAttention\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        if isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "        if isinstance(module, MultiheadAttention):\n",
        "            module.q_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            module.k_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            module.v_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "        if isinstance(module, nn.RNNBase):\n",
        "            for name, param in module.named_parameters():\n",
        "                if \"weight\" in name or \"bias\" in name:\n",
        "                    param.data.uniform_(-0.1, 0.1)\n",
        "            \n",
        "    # weight initialization\n",
        "    model.apply(init_params)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce5n4eS7NQNy"
      },
      "source": [
        "## Architecture Related Configuration\n",
        "\n",
        "For strong baseline, please refer to the hyperparameters for *transformer-base* in Table 3 in [Attention is all you need](#vaswani2017)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Cyn30VoGNT6N"
      },
      "outputs": [],
      "source": [
        "arch_args = Namespace(\n",
        "    encoder_embed_dim=256,\n",
        "    encoder_ffn_embed_dim=1024,\n",
        "    encoder_layers=4,\n",
        "    decoder_embed_dim=256,\n",
        "    decoder_ffn_embed_dim=1024,\n",
        "    decoder_layers=4,\n",
        "    share_decoder_input_output_embed=True,\n",
        "    dropout=0.15,\n",
        ")\n",
        "\n",
        "# HINT: these patches on parameters for Transformer\n",
        "def add_transformer_args(args):\n",
        "    args.encoder_attention_heads=4\n",
        "    args.encoder_normalize_before=True\n",
        "    \n",
        "    args.decoder_attention_heads=4\n",
        "    args.decoder_normalize_before=True\n",
        "    \n",
        "    args.activation_fn=\"relu\"\n",
        "    args.max_source_positions=1024\n",
        "    args.max_target_positions=1024\n",
        "    \n",
        "    # patches on default parameters for Transformer (those not set above)\n",
        "    from fairseq.models.transformer import base_architecture\n",
        "    base_architecture(arch_args)\n",
        "\n",
        "add_transformer_args(arch_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Nbb76QLCNZZZ"
      },
      "outputs": [],
      "source": [
        "if config.use_wandb:\n",
        "    wandb.config.update(vars(arch_args))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "7ZWfxsCDNatH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 12:37:19 | INFO | hw5.seq2seq | Seq2Seq(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(7992, 256, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-3): 4 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(7992, 256, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-3): 4 x TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    (output_projection): Linear(in_features=256, out_features=7992, bias=False)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = build_model(arch_args, task)\n",
        "logger.info(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHll7GRNNdqc"
      },
      "source": [
        "# Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUB9f1WCNgMH"
      },
      "source": [
        "## Loss: Label Smoothing Regularization\n",
        "* let the model learn to generate less concentrated distribution, and prevent over-confidence\n",
        "* sometimes the ground truth may not be the only answer. thus, when calculating loss, we reserve some probability for incorrect labels\n",
        "* avoids overfitting\n",
        "\n",
        "code [source](https://fairseq.readthedocs.io/en/latest/_modules/fairseq/criterions/label_smoothed_cross_entropy.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "IgspdJn0NdYF"
      },
      "outputs": [],
      "source": [
        "class LabelSmoothedCrossEntropyCriterion(nn.Module):\n",
        "    def __init__(self, smoothing, ignore_index=None, reduce=True):\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "        self.ignore_index = ignore_index\n",
        "        self.reduce = reduce\n",
        "    \n",
        "    def forward(self, lprobs, target):\n",
        "        if target.dim() == lprobs.dim() - 1:\n",
        "            target = target.unsqueeze(-1)\n",
        "        # nll: Negative log likelihood，the cross-entropy when target is one-hot. following line is same as F.nll_loss\n",
        "        nll_loss = -lprobs.gather(dim=-1, index=target)\n",
        "        #  reserve some probability for other labels. thus when calculating cross-entropy, \n",
        "        # equivalent to summing the log probs of all labels\n",
        "        smooth_loss = -lprobs.sum(dim=-1, keepdim=True)\n",
        "        if self.ignore_index is not None:\n",
        "            pad_mask = target.eq(self.ignore_index)\n",
        "            nll_loss.masked_fill_(pad_mask, 0.0)\n",
        "            smooth_loss.masked_fill_(pad_mask, 0.0)\n",
        "        else:\n",
        "            nll_loss = nll_loss.squeeze(-1)\n",
        "            smooth_loss = smooth_loss.squeeze(-1)\n",
        "        if self.reduce:\n",
        "            nll_loss = nll_loss.sum()\n",
        "            smooth_loss = smooth_loss.sum()\n",
        "        # when calculating cross-entropy, add the loss of other labels\n",
        "        eps_i = self.smoothing / lprobs.size(-1)\n",
        "        loss = (1.0 - self.smoothing) * nll_loss + eps_i * smooth_loss\n",
        "        return loss\n",
        "\n",
        "# generally, 0.1 is good enough\n",
        "criterion = LabelSmoothedCrossEntropyCriterion(\n",
        "    smoothing=0.1,\n",
        "    ignore_index=task.target_dictionary.pad(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRalDto2NkJJ"
      },
      "source": [
        "## Optimizer: Adam + lr scheduling\n",
        "Inverse square root scheduling is important to the stability when training Transformer. It's later used on RNN as well.\n",
        "Update the learning rate according to the following equation. Linearly increase the first stage, then decay proportionally to the inverse square root of timestep.\n",
        "$$lrate = d_{\\text{model}}^{-0.5}\\cdot\\min({step\\_num}^{-0.5},{step\\_num}\\cdot{warmup\\_steps}^{-1.5})$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "sS7tQj1ROBYm"
      },
      "outputs": [],
      "source": [
        "def get_rate(d_model, step_num, warmup_step):\n",
        "    # TODO: Change lr from constant to the equation shown above\n",
        "    # lr = 0.001\n",
        "    lr = (d_model**(-0.5) * min(step_num**(-0.5), step_num*(warmup_step**(-1.5))))\n",
        "    return lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "J8hoAjHPNkh3"
      },
      "outputs": [],
      "source": [
        "class NoamOpt:\n",
        "    \"Optim wrapper that implements rate.\"\n",
        "    def __init__(self, model_size, factor, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.factor = factor\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "    \n",
        "    @property\n",
        "    def param_groups(self):\n",
        "        return self.optimizer.param_groups\n",
        "        \n",
        "    def multiply_grads(self, c):\n",
        "        \"\"\"Multiplies grads by a constant *c*.\"\"\"                \n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is not None:\n",
        "                    p.grad.data.mul_(c)\n",
        "        \n",
        "    def step(self):\n",
        "        \"Update parameters and rate\"\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def rate(self, step = None):\n",
        "        \"Implement `lrate` above\"\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return 0 if not step else self.factor * get_rate(self.model_size, step, self.warmup)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFJlkOMONsc6"
      },
      "source": [
        "## Scheduling Visualized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "A135fwPCNrQs"
      },
      "outputs": [],
      "source": [
        "optimizer = NoamOpt(\n",
        "    model_size=arch_args.encoder_embed_dim, \n",
        "    factor=config.lr_factor, \n",
        "    warmup=config.lr_warmup, \n",
        "    optimizer=torch.optim.AdamW(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.0001))\n",
        "# plt.plot(np.arange(1, 100000), [optimizer.rate(i) for i in range(1, 100000)])\n",
        "# plt.legend([f\"{optimizer.model_size}:{optimizer.warmup}\"])\n",
        "None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOR0g-cVO5ZO"
      },
      "source": [
        "# Training Procedure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-0ZjbK3O8Iv"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "foal3xM1O404"
      },
      "outputs": [],
      "source": [
        "from fairseq.data import iterators\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "gnorms = []\n",
        "def train_one_epoch(epoch_itr, model, task, criterion, optimizer, accum_steps=1):\n",
        "    itr = epoch_itr.next_epoch_itr(shuffle=True)\n",
        "    itr = iterators.GroupedIterator(itr, accum_steps) # gradient accumulation: update every accum_steps samples\n",
        "    \n",
        "    stats = {\"loss\": []}\n",
        "    scaler = GradScaler() # automatic mixed precision (amp) \n",
        "    \n",
        "    model.train()\n",
        "    progress = tqdm.tqdm(itr, desc=f\"train epoch {epoch_itr.epoch}\", leave=False)\n",
        "    for samples in progress:\n",
        "        model.zero_grad()\n",
        "        accum_loss = 0\n",
        "        sample_size = 0\n",
        "        # gradient accumulation: update every accum_steps samples\n",
        "        for i, sample in enumerate(samples):\n",
        "            if i == 1:\n",
        "                # emptying the CUDA cache after the first step can reduce the chance of OOM\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            sample = utils.move_to_cuda(sample, device=device)\n",
        "            target = sample[\"target\"]\n",
        "            sample_size_i = sample[\"ntokens\"]\n",
        "            sample_size += sample_size_i\n",
        "            \n",
        "            # mixed precision training\n",
        "            with autocast():\n",
        "                net_output = model.forward(**sample[\"net_input\"])\n",
        "                lprobs = F.log_softmax(net_output[0], -1)            \n",
        "                loss = criterion(lprobs.view(-1, lprobs.size(-1)), target.view(-1))\n",
        "                \n",
        "                # logging\n",
        "                accum_loss += loss.item()\n",
        "                # back-prop\n",
        "                scaler.scale(loss).backward()                \n",
        "        \n",
        "        scaler.unscale_(optimizer)\n",
        "        optimizer.multiply_grads(1 / (sample_size or 1.0)) # (sample_size or 1.0) handles the case of a zero gradient\n",
        "        gnorm = nn.utils.clip_grad_norm_(model.parameters(), config.clip_norm) # grad norm clipping prevents gradient exploding\n",
        "        with open('gnorm.txt', 'a') as gtf:\n",
        "            gtf.write(f\"{gnorm.item()}\\n\")\n",
        "        gnorms.append(gnorm.cpu().item())\n",
        "        \n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        \n",
        "        # logging\n",
        "        loss_print = accum_loss/sample_size\n",
        "        stats[\"loss\"].append(loss_print)\n",
        "        progress.set_postfix(loss=loss_print)\n",
        "        if config.use_wandb:\n",
        "            wandb.log({\n",
        "                \"train/loss\": loss_print,\n",
        "                \"train/grad_norm\": gnorm.item(),\n",
        "                \"train/lr\": optimizer.rate(),\n",
        "                \"train/sample_size\": sample_size,\n",
        "            })\n",
        "        \n",
        "    loss_print = np.mean(stats[\"loss\"])\n",
        "    logger.info(f\"training loss: {loss_print:.4f}\")\n",
        "    return stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt1lX3DRO_yU"
      },
      "source": [
        "## Validation & Inference\n",
        "To prevent overfitting, validation is required every epoch to validate the performance on unseen data.\n",
        "- the procedure is essensially same as training, with the addition of inference step\n",
        "- after validation we can save the model weights\n",
        "\n",
        "Validation loss alone cannot describe the actual performance of the model\n",
        "- Directly produce translation hypotheses based on current model, then calculate BLEU with the reference translation\n",
        "- We can also manually examine the hypotheses' quality\n",
        "- We use fairseq's sequence generator for beam search to generate translation hypotheses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "2og80HYQPAKq"
      },
      "outputs": [],
      "source": [
        "# fairseq's beam search generator\n",
        "# given model and input seqeunce, produce translation hypotheses by beam search\n",
        "sequence_generator = task.build_generator([model], config)\n",
        "\n",
        "def decode(toks, dictionary):\n",
        "    # convert from Tensor to human readable sentence\n",
        "    s = dictionary.string(\n",
        "        toks.int().cpu(),\n",
        "        config.post_process,\n",
        "    )\n",
        "    return s if s else \"<unk>\"\n",
        "\n",
        "def inference_step(sample, model):\n",
        "    gen_out = sequence_generator.generate([model], sample)\n",
        "    srcs = []\n",
        "    hyps = []\n",
        "    refs = []\n",
        "    for i in range(len(gen_out)):\n",
        "        # for each sample, collect the input, hypothesis and reference, later be used to calculate BLEU\n",
        "        srcs.append(decode(\n",
        "            utils.strip_pad(sample[\"net_input\"][\"src_tokens\"][i], task.source_dictionary.pad()), \n",
        "            task.source_dictionary,\n",
        "        ))\n",
        "        hyps.append(decode(\n",
        "            gen_out[i][0][\"tokens\"], # 0 indicates using the top hypothesis in beam\n",
        "            task.target_dictionary,\n",
        "        ))\n",
        "        refs.append(decode(\n",
        "            utils.strip_pad(sample[\"target\"][i], task.target_dictionary.pad()), \n",
        "            task.target_dictionary,\n",
        "        ))\n",
        "    return srcs, hyps, refs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "y1o7LeDkPDsd"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import sacrebleu\n",
        "\n",
        "def validate(model, task, criterion, log_to_wandb=True):\n",
        "    logger.info('begin validation')\n",
        "    itr = load_data_iterator(task, \"valid\", 1, config.max_tokens, config.num_workers).next_epoch_itr(shuffle=False)\n",
        "    \n",
        "    stats = {\"loss\":[], \"bleu\": 0, \"srcs\":[], \"hyps\":[], \"refs\":[]}\n",
        "    srcs = []\n",
        "    hyps = []\n",
        "    refs = []\n",
        "    \n",
        "    model.eval()\n",
        "    progress = tqdm.tqdm(itr, desc=f\"validation\", leave=False)\n",
        "    with torch.no_grad():\n",
        "        for i, sample in enumerate(progress):\n",
        "            # validation loss\n",
        "            sample = utils.move_to_cuda(sample, device=device)\n",
        "            net_output = model.forward(**sample[\"net_input\"])\n",
        "\n",
        "            lprobs = F.log_softmax(net_output[0], -1)\n",
        "            target = sample[\"target\"]\n",
        "            sample_size = sample[\"ntokens\"]\n",
        "            loss = criterion(lprobs.view(-1, lprobs.size(-1)), target.view(-1)) / sample_size\n",
        "            progress.set_postfix(valid_loss=loss.item())\n",
        "            stats[\"loss\"].append(loss)\n",
        "            \n",
        "            # do inference\n",
        "            s, h, r = inference_step(sample, model)\n",
        "            srcs.extend(s)\n",
        "            hyps.extend(h)\n",
        "            refs.extend(r)\n",
        "            \n",
        "    tok = 'zh' if task.cfg.target_lang == 'zh' else '13a'\n",
        "    stats[\"loss\"] = torch.stack(stats[\"loss\"]).mean().item()\n",
        "    stats[\"bleu\"] = sacrebleu.corpus_bleu(hyps, [refs], tokenize=tok) # 計算BLEU score\n",
        "    stats[\"srcs\"] = srcs\n",
        "    stats[\"hyps\"] = hyps\n",
        "    stats[\"refs\"] = refs\n",
        "    \n",
        "    if config.use_wandb and log_to_wandb:\n",
        "        wandb.log({\n",
        "            \"valid/loss\": stats[\"loss\"],\n",
        "            \"valid/bleu\": stats[\"bleu\"].score,\n",
        "        }, commit=False)\n",
        "    \n",
        "    showid = np.random.randint(len(hyps))\n",
        "    logger.info(\"example source: \" + srcs[showid])\n",
        "    logger.info(\"example hypothesis: \" + hyps[showid])\n",
        "    logger.info(\"example reference: \" + refs[showid])\n",
        "    \n",
        "    # show bleu results\n",
        "    logger.info(f\"validation loss:\\t{stats['loss']:.4f}\")\n",
        "    logger.info(stats[\"bleu\"].format())\n",
        "    return stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sRF6nd4PGEE"
      },
      "source": [
        "# Save and Load Model Weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "edBuLlkuPGr9"
      },
      "outputs": [],
      "source": [
        "def validate_and_save(model, task, criterion, optimizer, epoch, save=True):   \n",
        "    stats = validate(model, task, criterion)\n",
        "    bleu = stats['bleu']\n",
        "    loss = stats['loss']\n",
        "    if save:\n",
        "        # save epoch checkpoints\n",
        "        savedir = Path(config.savedir).absolute()\n",
        "        savedir.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        check = {\n",
        "            \"model\": model.state_dict(),\n",
        "            \"stats\": {\"bleu\": bleu.score, \"loss\": loss},\n",
        "            \"optim\": {\"step\": optimizer._step}\n",
        "        }\n",
        "        torch.save(check, savedir/f\"checkpoint{epoch}.pt\")\n",
        "        shutil.copy(savedir/f\"checkpoint{epoch}.pt\", savedir/f\"checkpoint_last.pt\")\n",
        "        logger.info(f\"saved epoch checkpoint: {savedir}/checkpoint{epoch}.pt\")\n",
        "    \n",
        "        # save epoch samples\n",
        "        with open(savedir/f\"samples{epoch}.{config.source_lang}-{config.target_lang}.txt\", \"w\") as f:\n",
        "            for s, h in zip(stats[\"srcs\"], stats[\"hyps\"]):\n",
        "                f.write(f\"{s}\\t{h}\\n\")\n",
        "\n",
        "        # get best valid bleu    \n",
        "        if getattr(validate_and_save, \"best_bleu\", 0) < bleu.score:\n",
        "            validate_and_save.best_bleu = bleu.score\n",
        "            torch.save(check, savedir/f\"checkpoint_best.pt\")\n",
        "            \n",
        "        del_file = savedir / f\"checkpoint{epoch - config.keep_last_epochs}.pt\"\n",
        "        if del_file.exists():\n",
        "            del_file.unlink()\n",
        "    \n",
        "    return stats\n",
        "\n",
        "def try_load_checkpoint(model, optimizer=None, name=None):\n",
        "    name = name if name else \"checkpoint_last.pt\"\n",
        "    checkpath = Path(config.savedir)/name\n",
        "    if checkpath.exists():\n",
        "        check = torch.load(checkpath)\n",
        "        model.load_state_dict(check[\"model\"])\n",
        "        stats = check[\"stats\"]\n",
        "        step = \"unknown\"\n",
        "        if optimizer != None:\n",
        "            optimizer._step = step = check[\"optim\"][\"step\"]\n",
        "        logger.info(f\"loaded checkpoint {checkpath}: step={step} loss={stats['loss']} bleu={stats['bleu']}\")\n",
        "    else:\n",
        "        logger.info(f\"no checkpoints found at {checkpath}!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyIFpibfPJ5u"
      },
      "source": [
        "# Main\n",
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "hu7RZbCUPKQr"
      },
      "outputs": [],
      "source": [
        "model = model.to(device=device)\n",
        "criterion = criterion.to(device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "5xxlJxU2PeAo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 12:37:19 | INFO | hw5.seq2seq | task: TranslationTask\n",
            "2023-04-14 12:37:19 | INFO | hw5.seq2seq | encoder: TransformerEncoder\n",
            "2023-04-14 12:37:19 | INFO | hw5.seq2seq | decoder: TransformerDecoder\n",
            "2023-04-14 12:37:19 | INFO | hw5.seq2seq | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2023-04-14 12:37:19 | INFO | hw5.seq2seq | optimizer: NoamOpt\n",
            "2023-04-14 12:37:19 | INFO | hw5.seq2seq | num. model params: 11,465,728 (num. trained: 11,465,728)\n",
            "2023-04-14 12:37:19 | INFO | hw5.seq2seq | max tokens per batch = 8192, accumulate steps = 2\n"
          ]
        }
      ],
      "source": [
        "logger.info(\"task: {}\".format(task.__class__.__name__))\n",
        "logger.info(\"encoder: {}\".format(model.encoder.__class__.__name__))\n",
        "logger.info(\"decoder: {}\".format(model.decoder.__class__.__name__))\n",
        "logger.info(\"criterion: {}\".format(criterion.__class__.__name__))\n",
        "logger.info(\"optimizer: {}\".format(optimizer.__class__.__name__))\n",
        "logger.info(\n",
        "    \"num. model params: {:,} (num. trained: {:,})\".format(\n",
        "        sum(p.numel() for p in model.parameters()),\n",
        "        sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
        "    )\n",
        ")\n",
        "logger.info(f\"max tokens per batch = {config.max_tokens}, accumulate steps = {config.accum_steps}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "MSPRqpQUPfaX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 12:37:48 | INFO | hw5.seq2seq | no checkpoints found at checkpoints\\transformer\\checkpoint_last.pt!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 12:41:22 | INFO | hw5.seq2seq | training loss: 6.9079\n",
            "2023-04-14 12:41:22 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 12:41:53 | INFO | hw5.seq2seq | example source: being a diplomat , then and now , is an incredible job , and i loved every minute of it i enjoyed the status of it .\n",
            "2023-04-14 12:41:53 | INFO | hw5.seq2seq | example hypothesis: 美國 , 是 , 是 , 大大大大大的 , 我很棒的 , 我很棒的 , 我很棒 。\n",
            "2023-04-14 12:41:53 | INFO | hw5.seq2seq | example reference: 過去和今日相較 , 外交官是個超棒的工作 , 我超愛在裡面工作的分分秒秒 。 我享受著所有的事件 。\n",
            "2023-04-14 12:41:53 | INFO | hw5.seq2seq | validation loss:\t5.7053\n",
            "2023-04-14 12:41:53 | INFO | hw5.seq2seq | BLEU = 1.26 16.6/3.2/0.7/0.2 (BP = 0.823 ratio = 0.837 hyp_len = 92426 ref_len = 110430)\n",
            "2023-04-14 12:41:53 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint1.pt\n",
            "2023-04-14 12:41:53 | INFO | hw5.seq2seq | end of epoch 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 12:45:30 | INFO | hw5.seq2seq | training loss: 5.2460\n",
            "2023-04-14 12:45:30 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 12:45:57 | INFO | hw5.seq2seq | example source: and now we know that there are hundreds of other sorts of cells , which can be very , very specific .\n",
            "2023-04-14 12:45:57 | INFO | hw5.seq2seq | example hypothesis: 現在 , 我們知道有數百個細胞 , 可以非常複雜 。\n",
            "2023-04-14 12:45:57 | INFO | hw5.seq2seq | example reference: 現在我們知道有上百種不同的細胞 , 負責非常特定的功能 。\n",
            "2023-04-14 12:45:57 | INFO | hw5.seq2seq | validation loss:\t4.7360\n",
            "2023-04-14 12:45:57 | INFO | hw5.seq2seq | BLEU = 9.72 42.3/17.5/7.8/3.6 (BP = 0.813 ratio = 0.829 hyp_len = 91513 ref_len = 110430)\n",
            "2023-04-14 12:45:58 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint2.pt\n",
            "2023-04-14 12:45:58 | INFO | hw5.seq2seq | end of epoch 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 12:49:34 | INFO | hw5.seq2seq | training loss: 4.6213\n",
            "2023-04-14 12:49:34 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 12:50:02 | INFO | hw5.seq2seq | example source: the big brands are some of the most important powers , powerful powers , in this country .\n",
            "2023-04-14 12:50:02 | INFO | hw5.seq2seq | example hypothesis: 最大的勇敢是最重要的力量 , 在這個國家 。\n",
            "2023-04-14 12:50:02 | INFO | hw5.seq2seq | example reference: 有名的速食品牌名列這個國家最有勢力 , 最有影響力的名單中 , 超市也是 。\n",
            "2023-04-14 12:50:02 | INFO | hw5.seq2seq | validation loss:\t4.3132\n",
            "2023-04-14 12:50:02 | INFO | hw5.seq2seq | BLEU = 14.06 45.5/21.3/10.8/5.8 (BP = 0.895 ratio = 0.901 hyp_len = 99443 ref_len = 110430)\n",
            "2023-04-14 12:50:02 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint3.pt\n",
            "2023-04-14 12:50:02 | INFO | hw5.seq2seq | end of epoch 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 12:53:40 | INFO | hw5.seq2seq | training loss: 4.3203\n",
            "2023-04-14 12:53:40 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 12:54:09 | INFO | hw5.seq2seq | example source: we'll pay you three dollars for it . \"\n",
            "2023-04-14 12:54:09 | INFO | hw5.seq2seq | example hypothesis: 我們會付你三塊錢 。 」\n",
            "2023-04-14 12:54:09 | INFO | hw5.seq2seq | example reference: 我們會用$3.00跟你買 。 」\n",
            "2023-04-14 12:54:09 | INFO | hw5.seq2seq | validation loss:\t4.1355\n",
            "2023-04-14 12:54:09 | INFO | hw5.seq2seq | BLEU = 15.41 49.2/23.9/12.7/7.0 (BP = 0.855 ratio = 0.865 hyp_len = 95469 ref_len = 110430)\n",
            "2023-04-14 12:54:09 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint4.pt\n",
            "2023-04-14 12:54:09 | INFO | hw5.seq2seq | end of epoch 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 12:57:54 | INFO | hw5.seq2seq | training loss: 4.1342\n",
            "2023-04-14 12:57:54 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 12:58:21 | INFO | hw5.seq2seq | example source: [protect your selfesteem] we have to catch our unhealthy psychological habits and change them .\n",
            "2023-04-14 12:58:21 | INFO | hw5.seq2seq | example hypothesis: 「 私人自我利益 」 , 我們必須抓到我們的心理習慣 , 改變他們 。\n",
            "2023-04-14 12:58:21 | INFO | hw5.seq2seq | example reference: 「 保護你的自尊心 」 我們需要改變不健康的心理習慣 。\n",
            "2023-04-14 12:58:21 | INFO | hw5.seq2seq | validation loss:\t3.9546\n",
            "2023-04-14 12:58:21 | INFO | hw5.seq2seq | BLEU = 17.50 53.3/27.1/14.7/8.4 (BP = 0.853 ratio = 0.863 hyp_len = 95268 ref_len = 110430)\n",
            "2023-04-14 12:58:21 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint5.pt\n",
            "2023-04-14 12:58:21 | INFO | hw5.seq2seq | end of epoch 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 13:01:59 | INFO | hw5.seq2seq | training loss: 3.9797\n",
            "2023-04-14 13:01:59 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 13:02:28 | INFO | hw5.seq2seq | example source: and then you get to the finishes , the subject of all of those \" go green \" articles , and on the scale of a house they almost make no difference at all .\n",
            "2023-04-14 13:02:28 | INFO | hw5.seq2seq | example hypothesis: 然後你到最後一家 , 所有這些 「 去綠色 」 的主題 , 和一間房子都差不多 。\n",
            "2023-04-14 13:02:28 | INFO | hw5.seq2seq | example reference: 再來是表面處理 。 所有那些談 「 綠化 」 的文章 , 都以此為主題 。 以一個房子的規模來看 , 表面處理幾乎沒有什麼影響 。\n",
            "2023-04-14 13:02:28 | INFO | hw5.seq2seq | validation loss:\t3.8325\n",
            "2023-04-14 13:02:28 | INFO | hw5.seq2seq | BLEU = 19.90 51.6/26.7/14.8/8.6 (BP = 0.974 ratio = 0.974 hyp_len = 107569 ref_len = 110430)\n",
            "2023-04-14 13:02:28 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint6.pt\n",
            "2023-04-14 13:02:28 | INFO | hw5.seq2seq | end of epoch 6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 13:06:08 | INFO | hw5.seq2seq | training loss: 3.8378\n",
            "2023-04-14 13:06:08 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 13:06:37 | INFO | hw5.seq2seq | example source: i'd like to try something new .\n",
            "2023-04-14 13:06:37 | INFO | hw5.seq2seq | example hypothesis: 我想嘗試新的東西 。\n",
            "2023-04-14 13:06:37 | INFO | hw5.seq2seq | example reference: 我想嘗試一個新東西 ,\n",
            "2023-04-14 13:06:37 | INFO | hw5.seq2seq | validation loss:\t3.7154\n",
            "2023-04-14 13:06:37 | INFO | hw5.seq2seq | BLEU = 21.05 53.0/28.1/15.8/9.3 (BP = 0.973 ratio = 0.973 hyp_len = 107475 ref_len = 110430)\n",
            "2023-04-14 13:06:37 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint7.pt\n",
            "2023-04-14 13:06:37 | INFO | hw5.seq2seq | end of epoch 7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 13:10:12 | INFO | hw5.seq2seq | training loss: 3.7396\n",
            "2023-04-14 13:10:12 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 13:10:39 | INFO | hw5.seq2seq | example source: but unfortunately , the same day in fact , shortly after birth the calf died .\n",
            "2023-04-14 13:10:39 | INFO | hw5.seq2seq | example hypothesis: 但不幸的是 , 相同的一天 , 事實上 , 在出生後 , 幾乎沒時間就死了 。\n",
            "2023-04-14 13:10:39 | INFO | hw5.seq2seq | example reference: 但 , 不幸的是 , 同一天事實上 , 是才出生後不久幼鯨就死了 。\n",
            "2023-04-14 13:10:39 | INFO | hw5.seq2seq | validation loss:\t3.6564\n",
            "2023-04-14 13:10:39 | INFO | hw5.seq2seq | BLEU = 21.64 55.7/30.1/17.3/10.4 (BP = 0.924 ratio = 0.926 hyp_len = 102301 ref_len = 110430)\n",
            "2023-04-14 13:10:39 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint8.pt\n",
            "2023-04-14 13:10:39 | INFO | hw5.seq2seq | end of epoch 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 13:14:13 | INFO | hw5.seq2seq | training loss: 3.6672\n",
            "2023-04-14 13:14:13 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 13:14:40 | INFO | hw5.seq2seq | example source: why are we conscious ?\n",
            "2023-04-14 13:14:40 | INFO | hw5.seq2seq | example hypothesis: 為什麼我們有意識 ?\n",
            "2023-04-14 13:14:40 | INFO | hw5.seq2seq | example reference: 為何我們擁有意識 ?\n",
            "2023-04-14 13:14:40 | INFO | hw5.seq2seq | validation loss:\t3.6092\n",
            "2023-04-14 13:14:40 | INFO | hw5.seq2seq | BLEU = 22.31 57.1/31.3/18.2/11.1 (BP = 0.911 ratio = 0.915 hyp_len = 100998 ref_len = 110430)\n",
            "2023-04-14 13:14:40 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint9.pt\n",
            "2023-04-14 13:14:40 | INFO | hw5.seq2seq | end of epoch 9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 13:18:16 | INFO | hw5.seq2seq | training loss: 3.6110\n",
            "2023-04-14 13:18:16 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 13:18:42 | INFO | hw5.seq2seq | example source: we who are diplomats , we are trained to deal with conflicts between states and issues between states .\n",
            "2023-04-14 13:18:42 | INFO | hw5.seq2seq | example hypothesis: 我們是外交人 , 我們被訓練要處理國家和國家之間的衝突 。\n",
            "2023-04-14 13:18:42 | INFO | hw5.seq2seq | example reference: 這就是我們:這些外交官 , 我們受過訓練以應付國家之間的衝突及問題\n",
            "2023-04-14 13:18:42 | INFO | hw5.seq2seq | validation loss:\t3.5725\n",
            "2023-04-14 13:18:42 | INFO | hw5.seq2seq | BLEU = 21.62 58.3/32.2/18.6/11.4 (BP = 0.862 ratio = 0.870 hyp_len = 96115 ref_len = 110430)\n",
            "2023-04-14 13:18:42 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint10.pt\n",
            "2023-04-14 13:18:42 | INFO | hw5.seq2seq | end of epoch 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 13:22:19 | INFO | hw5.seq2seq | training loss: 3.5673\n",
            "2023-04-14 13:22:19 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 13:22:43 | INFO | hw5.seq2seq | example source: this is actual 3d points with two to three millimeter accuracy .\n",
            "2023-04-14 13:22:43 | INFO | hw5.seq2seq | example hypothesis: 這是實際的3d點 , 有2到3毫米的準確度 。\n",
            "2023-04-14 13:22:43 | INFO | hw5.seq2seq | example reference: 實際上這是由高達兩到三百萬個雷射光點所呈現的精確效果 。\n",
            "2023-04-14 13:22:43 | INFO | hw5.seq2seq | validation loss:\t3.5367\n",
            "2023-04-14 13:22:43 | INFO | hw5.seq2seq | BLEU = 22.45 59.5/33.1/19.5/11.9 (BP = 0.863 ratio = 0.872 hyp_len = 96248 ref_len = 110430)\n",
            "2023-04-14 13:22:43 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint11.pt\n",
            "2023-04-14 13:22:43 | INFO | hw5.seq2seq | end of epoch 11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 13:26:20 | INFO | hw5.seq2seq | training loss: 3.5292\n",
            "2023-04-14 13:26:20 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 13:26:49 | INFO | hw5.seq2seq | example source: you're walking around ; your car has 12 microprocessors .\n",
            "2023-04-14 13:26:49 | INFO | hw5.seq2seq | example hypothesis: 你在四處走動 , 你的車子有12個微處理器 。\n",
            "2023-04-14 13:26:49 | INFO | hw5.seq2seq | example reference: 四處走走 , 你的汽車里就有12個微處理器 。\n",
            "2023-04-14 13:26:49 | INFO | hw5.seq2seq | validation loss:\t3.5160\n",
            "2023-04-14 13:26:49 | INFO | hw5.seq2seq | BLEU = 22.94 56.9/31.4/18.3/11.2 (BP = 0.933 ratio = 0.935 hyp_len = 103273 ref_len = 110430)\n",
            "2023-04-14 13:26:49 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint12.pt\n",
            "2023-04-14 13:26:49 | INFO | hw5.seq2seq | end of epoch 12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 13:30:26 | INFO | hw5.seq2seq | training loss: 3.5029\n",
            "2023-04-14 13:30:26 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 13:30:52 | INFO | hw5.seq2seq | example source: they want to the only way for them to survive is to get a printing press .\n",
            "2023-04-14 13:30:52 | INFO | hw5.seq2seq | example hypothesis: 他們希望他們能存活下來的唯一方法就是讓印刷機 。\n",
            "2023-04-14 13:30:52 | INFO | hw5.seq2seq | example reference: 他們存活下去的唯一出路 , 目的就是要買一台印刷機\n",
            "2023-04-14 13:30:52 | INFO | hw5.seq2seq | validation loss:\t3.4789\n",
            "2023-04-14 13:30:52 | INFO | hw5.seq2seq | BLEU = 23.29 58.6/32.6/19.1/11.7 (BP = 0.911 ratio = 0.915 hyp_len = 100991 ref_len = 110430)\n",
            "2023-04-14 13:30:53 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint13.pt\n",
            "2023-04-14 13:30:53 | INFO | hw5.seq2seq | end of epoch 13\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 13:34:31 | INFO | hw5.seq2seq | training loss: 3.4727\n",
            "2023-04-14 13:34:31 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 13:34:57 | INFO | hw5.seq2seq | example source: now about half the audience has their left hand up . why is that ?\n",
            "2023-04-14 13:34:57 | INFO | hw5.seq2seq | example hypothesis: 大約有一半的觀眾有左手 , 為什麼會這樣 ?\n",
            "2023-04-14 13:34:57 | INFO | hw5.seq2seq | example reference: 大約有一半的人舉起來的是左手 , 那<unk>阿<unk><unk> ?\n",
            "2023-04-14 13:34:57 | INFO | hw5.seq2seq | validation loss:\t3.4701\n",
            "2023-04-14 13:34:57 | INFO | hw5.seq2seq | BLEU = 23.98 57.7/32.2/19.0/11.8 (BP = 0.945 ratio = 0.946 hyp_len = 104505 ref_len = 110430)\n",
            "2023-04-14 13:34:58 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint14.pt\n",
            "2023-04-14 13:34:58 | INFO | hw5.seq2seq | end of epoch 14\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 13:38:35 | INFO | hw5.seq2seq | training loss: 3.4527\n",
            "2023-04-14 13:38:35 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 13:39:01 | INFO | hw5.seq2seq | example source: god , the one who rules the entire universe , wants my bread ? \"\n",
            "2023-04-14 13:39:01 | INFO | hw5.seq2seq | example hypothesis: 上帝 , 統治整個宇宙的人 , 想要我的麵包 ? 」\n",
            "2023-04-14 13:39:01 | INFO | hw5.seq2seq | example reference: 上帝 , 一個掌管全宇宙的神 , 要我的麵包 ? 」\n",
            "2023-04-14 13:39:01 | INFO | hw5.seq2seq | validation loss:\t3.4573\n",
            "2023-04-14 13:39:01 | INFO | hw5.seq2seq | BLEU = 23.47 58.9/33.0/19.5/12.1 (BP = 0.902 ratio = 0.907 hyp_len = 100113 ref_len = 110430)\n",
            "2023-04-14 13:39:01 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint15.pt\n",
            "2023-04-14 13:39:01 | INFO | hw5.seq2seq | end of epoch 15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 13:42:39 | INFO | hw5.seq2seq | training loss: 3.4308\n",
            "2023-04-14 13:42:39 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 13:43:06 | INFO | hw5.seq2seq | example source: and not only in the states , but in any country , in any economy .\n",
            "2023-04-14 13:43:06 | INFO | hw5.seq2seq | example hypothesis: 不僅在美國 , 在任何國家 , 在任何國家 , 任何經濟體中 。\n",
            "2023-04-14 13:43:06 | INFO | hw5.seq2seq | example reference: 而且不止在美國 , 在任何國家、任何經濟體系裏都有這樣的效益\n",
            "2023-04-14 13:43:06 | INFO | hw5.seq2seq | validation loss:\t3.4412\n",
            "2023-04-14 13:43:06 | INFO | hw5.seq2seq | BLEU = 24.40 57.3/32.0/18.9/11.7 (BP = 0.966 ratio = 0.966 hyp_len = 106701 ref_len = 110430)\n",
            "2023-04-14 13:43:06 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint16.pt\n",
            "2023-04-14 13:43:07 | INFO | hw5.seq2seq | end of epoch 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 13:46:44 | INFO | hw5.seq2seq | training loss: 3.4149\n",
            "2023-04-14 13:46:44 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 13:47:11 | INFO | hw5.seq2seq | example source: we learned that thousands of people wanted to tell us their prices .\n",
            "2023-04-14 13:47:11 | INFO | hw5.seq2seq | example hypothesis: 我們發現成千上萬人想要告訴我們他們的價格 。\n",
            "2023-04-14 13:47:11 | INFO | hw5.seq2seq | example reference: 我們發現 , 有上千人想要告訴我們他們的價格 。\n",
            "2023-04-14 13:47:11 | INFO | hw5.seq2seq | validation loss:\t3.4366\n",
            "2023-04-14 13:47:11 | INFO | hw5.seq2seq | BLEU = 24.35 58.2/32.7/19.4/12.1 (BP = 0.942 ratio = 0.943 hyp_len = 104157 ref_len = 110430)\n",
            "2023-04-14 13:47:11 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint17.pt\n",
            "2023-04-14 13:47:11 | INFO | hw5.seq2seq | end of epoch 17\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 13:50:48 | INFO | hw5.seq2seq | training loss: 3.3979\n",
            "2023-04-14 13:50:48 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 13:51:13 | INFO | hw5.seq2seq | example source: we're in christchurch , where people have lived through a devastating natural disaster and recovered .\n",
            "2023-04-14 13:51:13 | INFO | hw5.seq2seq | example hypothesis: 我們在基督教堂 , 在那裡 , 人們經歷了嚴重的自然災難 , 並復原 。\n",
            "2023-04-14 13:51:13 | INFO | hw5.seq2seq | example reference: 我們在基督城 , 這裡的人度過了嚴重的天然災難且從中恢復了 。\n",
            "2023-04-14 13:51:13 | INFO | hw5.seq2seq | validation loss:\t3.4179\n",
            "2023-04-14 13:51:13 | INFO | hw5.seq2seq | BLEU = 24.16 59.2/33.3/19.7/12.3 (BP = 0.918 ratio = 0.921 hyp_len = 101757 ref_len = 110430)\n",
            "2023-04-14 13:51:13 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint18.pt\n",
            "2023-04-14 13:51:13 | INFO | hw5.seq2seq | end of epoch 18\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 13:54:53 | INFO | hw5.seq2seq | training loss: 3.3830\n",
            "2023-04-14 13:54:53 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 13:55:20 | INFO | hw5.seq2seq | example source: four years ago , a security researcher , or , as most people would call it , a hacker , found a way to literally make atms throw money at him .\n",
            "2023-04-14 13:55:20 | INFO | hw5.seq2seq | example hypothesis: 四年前 , 一位安全研究員 , 或者 , 如大多數人所說的 , 駭客 , 發現了一種方法 , 能讓ms扔錢給他 。\n",
            "2023-04-14 13:55:20 | INFO | hw5.seq2seq | example reference: 四年前 , 一位安全研究員 , 或者 , 大部分人會稱之為駭客 , 找到一個讓自動提款機向他吐鈔的方法 ,\n",
            "2023-04-14 13:55:20 | INFO | hw5.seq2seq | validation loss:\t3.4101\n",
            "2023-04-14 13:55:20 | INFO | hw5.seq2seq | BLEU = 24.57 57.6/32.3/19.1/11.8 (BP = 0.966 ratio = 0.966 hyp_len = 106710 ref_len = 110430)\n",
            "2023-04-14 13:55:20 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint19.pt\n",
            "2023-04-14 13:55:20 | INFO | hw5.seq2seq | end of epoch 19\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 13:58:57 | INFO | hw5.seq2seq | training loss: 3.3700\n",
            "2023-04-14 13:58:57 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 13:59:23 | INFO | hw5.seq2seq | example source: when i got to guatemala in 1995 , i heard of a case of a massacre that happened on may 14 , 1982 , where the army came in , killed the men , and took the women and children in helicopters to an unknown location .\n",
            "2023-04-14 13:59:23 | INFO | hw5.seq2seq | example hypothesis: 當我在1995年到瓜地馬拉時 , 我聽到一個大屠殺案例 , 發生在1982年5月14日 , 軍隊進來 , 殺死了男性 , 帶著直升機的孩子到未知的地點 。\n",
            "2023-04-14 13:59:23 | INFO | hw5.seq2seq | example reference: 當我1995年到達瓜地馬拉時我聽說有一件在1982年5月14日發生的屠殺軍隊去到那裡 , 殺掉男人將女人和小孩子們用直升機載到不明地點\n",
            "2023-04-14 13:59:23 | INFO | hw5.seq2seq | validation loss:\t3.4039\n",
            "2023-04-14 13:59:23 | INFO | hw5.seq2seq | BLEU = 24.28 59.2/33.4/19.8/12.4 (BP = 0.919 ratio = 0.922 hyp_len = 101854 ref_len = 110430)\n",
            "2023-04-14 13:59:23 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint20.pt\n",
            "2023-04-14 13:59:23 | INFO | hw5.seq2seq | end of epoch 20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 14:03:00 | INFO | hw5.seq2seq | training loss: 3.3573\n",
            "2023-04-14 14:03:00 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 14:03:26 | INFO | hw5.seq2seq | example source: so , where do we look for inspiration ? we've still got bill clinton .\n",
            "2023-04-14 14:03:26 | INFO | hw5.seq2seq | example hypothesis: 所以 , 我們要找什麼靈感 ? 我們仍然有比爾·克林頓 。\n",
            "2023-04-14 14:03:26 | INFO | hw5.seq2seq | example reference: 那麼 , 我們還能從何處尋求啟發 ? 我們總還有比爾.柯林頓 。\n",
            "2023-04-14 14:03:26 | INFO | hw5.seq2seq | validation loss:\t3.3982\n",
            "2023-04-14 14:03:26 | INFO | hw5.seq2seq | BLEU = 24.64 58.9/33.3/19.8/12.4 (BP = 0.936 ratio = 0.938 hyp_len = 103533 ref_len = 110430)\n",
            "2023-04-14 14:03:26 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint21.pt\n",
            "2023-04-14 14:03:26 | INFO | hw5.seq2seq | end of epoch 21\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 14:07:04 | INFO | hw5.seq2seq | training loss: 3.3453\n",
            "2023-04-14 14:07:04 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 14:07:31 | INFO | hw5.seq2seq | example source: jf: you know , i was thinking this morning , i don't even know what i would do without my women friends .\n",
            "2023-04-14 14:07:31 | INFO | hw5.seq2seq | example hypothesis: jf:你知道嗎 ? 我今天早上在想 , 我甚至不知道我會怎麼做 , 沒有我的女朋友 。\n",
            "2023-04-14 14:07:31 | INFO | hw5.seq2seq | example reference: jf:今早我在想我根本無法想像沒有我的女性朋友會怎樣\n",
            "2023-04-14 14:07:31 | INFO | hw5.seq2seq | validation loss:\t3.3829\n",
            "2023-04-14 14:07:31 | INFO | hw5.seq2seq | BLEU = 24.86 58.7/33.3/19.8/12.5 (BP = 0.943 ratio = 0.944 hyp_len = 104280 ref_len = 110430)\n",
            "2023-04-14 14:07:31 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint22.pt\n",
            "2023-04-14 14:07:31 | INFO | hw5.seq2seq | end of epoch 22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 14:11:06 | INFO | hw5.seq2seq | training loss: 3.3363\n",
            "2023-04-14 14:11:06 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 14:11:34 | INFO | hw5.seq2seq | example source: and i said , \" yeah . \"\n",
            "2023-04-14 14:11:34 | INFO | hw5.seq2seq | example hypothesis: 我說: 「 是啊 。 」\n",
            "2023-04-14 14:11:34 | INFO | hw5.seq2seq | example reference: 我說: 「 對啊 。 」\n",
            "2023-04-14 14:11:34 | INFO | hw5.seq2seq | validation loss:\t3.3866\n",
            "2023-04-14 14:11:34 | INFO | hw5.seq2seq | BLEU = 24.91 58.1/32.8/19.5/12.2 (BP = 0.959 ratio = 0.960 hyp_len = 106033 ref_len = 110430)\n",
            "2023-04-14 14:11:34 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint23.pt\n",
            "2023-04-14 14:11:34 | INFO | hw5.seq2seq | end of epoch 23\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 14:15:11 | INFO | hw5.seq2seq | training loss: 3.3274\n",
            "2023-04-14 14:15:11 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 14:15:38 | INFO | hw5.seq2seq | example source: now i can feel a sensation of delight and beauty if i look at that eye .\n",
            "2023-04-14 14:15:38 | INFO | hw5.seq2seq | example hypothesis: 如果我看著那眼睛 , 我可以感受到喜悅和美麗的感覺 。\n",
            "2023-04-14 14:15:38 | INFO | hw5.seq2seq | example reference: 看著眼睛我感受到快樂和美 。\n",
            "2023-04-14 14:15:38 | INFO | hw5.seq2seq | validation loss:\t3.3824\n",
            "2023-04-14 14:15:38 | INFO | hw5.seq2seq | BLEU = 25.22 57.6/32.5/19.4/12.2 (BP = 0.977 ratio = 0.977 hyp_len = 107936 ref_len = 110430)\n",
            "2023-04-14 14:15:38 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint24.pt\n",
            "2023-04-14 14:15:38 | INFO | hw5.seq2seq | end of epoch 24\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 14:19:15 | INFO | hw5.seq2seq | training loss: 3.3199\n",
            "2023-04-14 14:19:15 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 14:19:42 | INFO | hw5.seq2seq | example source: and he has a sweetheart , but she is american woman , not chinese .\n",
            "2023-04-14 14:19:42 | INFO | hw5.seq2seq | example hypothesis: 他有甜美的 , 但她是美國女性 , 而不是中國人 。\n",
            "2023-04-14 14:19:42 | INFO | hw5.seq2seq | example reference: 他有一個女朋友 , 但她不是中國人 , 而是一個美國女孩 。\n",
            "2023-04-14 14:19:42 | INFO | hw5.seq2seq | validation loss:\t3.3669\n",
            "2023-04-14 14:19:42 | INFO | hw5.seq2seq | BLEU = 25.03 58.9/33.4/19.9/12.5 (BP = 0.945 ratio = 0.947 hyp_len = 104555 ref_len = 110430)\n",
            "2023-04-14 14:19:42 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint25.pt\n",
            "2023-04-14 14:19:42 | INFO | hw5.seq2seq | end of epoch 25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 14:23:20 | INFO | hw5.seq2seq | training loss: 3.3118\n",
            "2023-04-14 14:23:20 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 14:23:46 | INFO | hw5.seq2seq | example source: that day , i discovered the power of fashion , and i've been in love with it ever since .\n",
            "2023-04-14 14:23:46 | INFO | hw5.seq2seq | example hypothesis: 那天 , 我發現時尚的力量 , 我從此就愛上了它 。\n",
            "2023-04-14 14:23:46 | INFO | hw5.seq2seq | example reference: 那天 , 我發現了時尚的力量 , 我從此就愛上了它 。\n",
            "2023-04-14 14:23:46 | INFO | hw5.seq2seq | validation loss:\t3.3641\n",
            "2023-04-14 14:23:46 | INFO | hw5.seq2seq | BLEU = 25.00 59.2/33.7/20.3/12.8 (BP = 0.932 ratio = 0.934 hyp_len = 103139 ref_len = 110430)\n",
            "2023-04-14 14:23:47 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint26.pt\n",
            "2023-04-14 14:23:47 | INFO | hw5.seq2seq | end of epoch 26\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 14:27:23 | INFO | hw5.seq2seq | training loss: 3.3020\n",
            "2023-04-14 14:27:23 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 14:27:50 | INFO | hw5.seq2seq | example source: now , the island of mauritius is a small island off the east coast of madagascar in the indian ocean , and it is the place where the dodo bird was discovered and extinguished , all within about 150 years .\n",
            "2023-04-14 14:27:50 | INFO | hw5.seq2seq | example hypothesis: 模里西斯島是一座小島 , 位於印度海岸的馬達加斯加東岸 , 在那裡 , dodododododo鳥被發現並且出名 , 大約在150年內 。\n",
            "2023-04-14 14:27:50 | INFO | hw5.seq2seq | example reference: 馬里提斯島是一個小島位於馬達加斯加島東部海域處在印度洋中 , 就是在這裡渡渡鳥被發現也滅絕了 , 僅在短短一百五十年間\n",
            "2023-04-14 14:27:50 | INFO | hw5.seq2seq | validation loss:\t3.3628\n",
            "2023-04-14 14:27:50 | INFO | hw5.seq2seq | BLEU = 24.86 58.5/33.1/19.6/12.3 (BP = 0.952 ratio = 0.953 hyp_len = 105234 ref_len = 110430)\n",
            "2023-04-14 14:27:50 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint27.pt\n",
            "2023-04-14 14:27:50 | INFO | hw5.seq2seq | end of epoch 27\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 14:31:26 | INFO | hw5.seq2seq | training loss: 3.2964\n",
            "2023-04-14 14:31:26 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 14:31:53 | INFO | hw5.seq2seq | example source: and the people that build things developers and governments they're naturally afraid of innovation , and they'd rather just use those forms that they know you'll respond to .\n",
            "2023-04-14 14:31:53 | INFO | hw5.seq2seq | example hypothesis: 那些建造東西的人 , 他們自然害怕創新 , 他們寧願用他們知道的形式來回應 。\n",
            "2023-04-14 14:31:53 | INFO | hw5.seq2seq | example reference: 蓋東西的人-開發者和政府-他們在本質上害怕創新他們寧願用這些他們知道你會如何回應的形式\n",
            "2023-04-14 14:31:53 | INFO | hw5.seq2seq | validation loss:\t3.3652\n",
            "2023-04-14 14:31:53 | INFO | hw5.seq2seq | BLEU = 24.70 59.6/34.0/20.3/12.8 (BP = 0.917 ratio = 0.920 hyp_len = 101598 ref_len = 110430)\n",
            "2023-04-14 14:31:53 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint28.pt\n",
            "2023-04-14 14:31:53 | INFO | hw5.seq2seq | end of epoch 28\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 14:35:31 | INFO | hw5.seq2seq | training loss: 3.2906\n",
            "2023-04-14 14:35:31 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 14:35:57 | INFO | hw5.seq2seq | example source: so you're probably all wondering: the cave .\n",
            "2023-04-14 14:35:57 | INFO | hw5.seq2seq | example hypothesis: 你可能都想知道:洞穴 。\n",
            "2023-04-14 14:35:57 | INFO | hw5.seq2seq | example reference: 所以你大概會猜想:那個洞穴中\n",
            "2023-04-14 14:35:57 | INFO | hw5.seq2seq | validation loss:\t3.3499\n",
            "2023-04-14 14:35:57 | INFO | hw5.seq2seq | BLEU = 25.08 59.2/33.6/20.1/12.7 (BP = 0.938 ratio = 0.940 hyp_len = 103835 ref_len = 110430)\n",
            "2023-04-14 14:35:57 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint29.pt\n",
            "2023-04-14 14:35:57 | INFO | hw5.seq2seq | end of epoch 29\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 14:39:33 | INFO | hw5.seq2seq | training loss: 3.2816\n",
            "2023-04-14 14:39:33 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 14:40:00 | INFO | hw5.seq2seq | example source: and that will get you very far .\n",
            "2023-04-14 14:40:00 | INFO | hw5.seq2seq | example hypothesis: 這會使你非常遙遠 。\n",
            "2023-04-14 14:40:00 | INFO | hw5.seq2seq | example reference: 這將對你受益無窮 。\n",
            "2023-04-14 14:40:00 | INFO | hw5.seq2seq | validation loss:\t3.3500\n",
            "2023-04-14 14:40:00 | INFO | hw5.seq2seq | BLEU = 25.29 58.4/33.2/19.9/12.5 (BP = 0.960 ratio = 0.961 hyp_len = 106086 ref_len = 110430)\n",
            "2023-04-14 14:40:00 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint30.pt\n",
            "2023-04-14 14:40:00 | INFO | hw5.seq2seq | end of epoch 30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 14:43:36 | INFO | hw5.seq2seq | training loss: 3.2760\n",
            "2023-04-14 14:43:36 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 14:44:02 | INFO | hw5.seq2seq | example source: one study asked people to estimate several statistics related to the scope of climate change .\n",
            "2023-04-14 14:44:02 | INFO | hw5.seq2seq | example hypothesis: 一項研究要求人們估計和氣候變遷的範圍有關的幾個統計數據 。\n",
            "2023-04-14 14:44:02 | INFO | hw5.seq2seq | example reference: 有一項研究要求受測者去估計幾項和氣候變遷範圍相關的統計數字 。\n",
            "2023-04-14 14:44:02 | INFO | hw5.seq2seq | validation loss:\t3.3530\n",
            "2023-04-14 14:44:02 | INFO | hw5.seq2seq | BLEU = 24.67 59.2/33.7/20.1/12.7 (BP = 0.924 ratio = 0.927 hyp_len = 102337 ref_len = 110430)\n",
            "2023-04-14 14:44:02 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint31.pt\n",
            "2023-04-14 14:44:02 | INFO | hw5.seq2seq | end of epoch 31\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 14:47:40 | INFO | hw5.seq2seq | training loss: 3.2700\n",
            "2023-04-14 14:47:40 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 14:48:05 | INFO | hw5.seq2seq | example source: it's not such an ominous thing .\n",
            "2023-04-14 14:48:05 | INFO | hw5.seq2seq | example hypothesis: 這並不是一件不可思議的事 。\n",
            "2023-04-14 14:48:05 | INFO | hw5.seq2seq | example reference: 這個任務也不是太糟糕\n",
            "2023-04-14 14:48:05 | INFO | hw5.seq2seq | validation loss:\t3.3607\n",
            "2023-04-14 14:48:05 | INFO | hw5.seq2seq | BLEU = 24.60 60.3/34.6/20.8/13.1 (BP = 0.895 ratio = 0.901 hyp_len = 99445 ref_len = 110430)\n",
            "2023-04-14 14:48:05 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint32.pt\n",
            "2023-04-14 14:48:05 | INFO | hw5.seq2seq | end of epoch 32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 14:51:42 | INFO | hw5.seq2seq | training loss: 3.2644\n",
            "2023-04-14 14:51:42 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 14:52:09 | INFO | hw5.seq2seq | example source: so . . .\n",
            "2023-04-14 14:52:09 | INFO | hw5.seq2seq | example hypothesis: 所以......\n",
            "2023-04-14 14:52:09 | INFO | hw5.seq2seq | example reference: 所以......\n",
            "2023-04-14 14:52:09 | INFO | hw5.seq2seq | validation loss:\t3.3428\n",
            "2023-04-14 14:52:09 | INFO | hw5.seq2seq | BLEU = 25.04 59.2/33.7/20.1/12.8 (BP = 0.936 ratio = 0.938 hyp_len = 103597 ref_len = 110430)\n",
            "2023-04-14 14:52:09 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint33.pt\n",
            "2023-04-14 14:52:09 | INFO | hw5.seq2seq | end of epoch 33\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 14:55:44 | INFO | hw5.seq2seq | training loss: 3.2585\n",
            "2023-04-14 14:55:44 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 14:56:11 | INFO | hw5.seq2seq | example source: and it's not just water that this works with .\n",
            "2023-04-14 14:56:11 | INFO | hw5.seq2seq | example hypothesis: 這不僅適用於水 。\n",
            "2023-04-14 14:56:11 | INFO | hw5.seq2seq | example reference: 然後它還不只是對水才有作用而已\n",
            "2023-04-14 14:56:11 | INFO | hw5.seq2seq | validation loss:\t3.3376\n",
            "2023-04-14 14:56:11 | INFO | hw5.seq2seq | BLEU = 25.68 58.8/33.6/20.3/12.9 (BP = 0.958 ratio = 0.959 hyp_len = 105929 ref_len = 110430)\n",
            "2023-04-14 14:56:11 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint34.pt\n",
            "2023-04-14 14:56:11 | INFO | hw5.seq2seq | end of epoch 34\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 14:59:47 | INFO | hw5.seq2seq | training loss: 3.2542\n",
            "2023-04-14 14:59:47 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 15:00:14 | INFO | hw5.seq2seq | example source: ok , i'm not the only one whistling here .\n",
            "2023-04-14 15:00:14 | INFO | hw5.seq2seq | example hypothesis: 好 , 我不是唯一吹口哨的人 。\n",
            "2023-04-14 15:00:14 | INFO | hw5.seq2seq | example reference: 好 , 我不是這裡唯一會吹口哨的人 。\n",
            "2023-04-14 15:00:14 | INFO | hw5.seq2seq | validation loss:\t3.3304\n",
            "2023-04-14 15:00:14 | INFO | hw5.seq2seq | BLEU = 25.44 59.3/33.9/20.3/12.8 (BP = 0.946 ratio = 0.947 hyp_len = 104626 ref_len = 110430)\n",
            "2023-04-14 15:00:14 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint35.pt\n",
            "2023-04-14 15:00:14 | INFO | hw5.seq2seq | end of epoch 35\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 15:03:51 | INFO | hw5.seq2seq | training loss: 3.2497\n",
            "2023-04-14 15:03:51 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 15:04:17 | INFO | hw5.seq2seq | example source: my father was literally born again .\n",
            "2023-04-14 15:04:17 | INFO | hw5.seq2seq | example hypothesis: 我父親真的再次出生 。\n",
            "2023-04-14 15:04:17 | INFO | hw5.seq2seq | example reference: 我爸爸真的重生了 。\n",
            "2023-04-14 15:04:17 | INFO | hw5.seq2seq | validation loss:\t3.3352\n",
            "2023-04-14 15:04:17 | INFO | hw5.seq2seq | BLEU = 25.68 58.9/33.6/20.2/12.8 (BP = 0.960 ratio = 0.960 hyp_len = 106059 ref_len = 110430)\n",
            "2023-04-14 15:04:17 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint36.pt\n",
            "2023-04-14 15:04:18 | INFO | hw5.seq2seq | end of epoch 36\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 15:07:53 | INFO | hw5.seq2seq | training loss: 3.2466\n",
            "2023-04-14 15:07:53 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 15:08:19 | INFO | hw5.seq2seq | example source: so what this means is that nature doesn't have to continually redesign the brain .\n",
            "2023-04-14 15:08:19 | INFO | hw5.seq2seq | example hypothesis: 這意味著大自然不需要不斷重新設計大腦 。\n",
            "2023-04-14 15:08:19 | INFO | hw5.seq2seq | example reference: 這意味著大自然不需持續重新設計大腦 。\n",
            "2023-04-14 15:08:19 | INFO | hw5.seq2seq | validation loss:\t3.3456\n",
            "2023-04-14 15:08:19 | INFO | hw5.seq2seq | BLEU = 25.21 60.4/34.8/21.0/13.4 (BP = 0.909 ratio = 0.913 hyp_len = 100856 ref_len = 110430)\n",
            "2023-04-14 15:08:19 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint37.pt\n",
            "2023-04-14 15:08:19 | INFO | hw5.seq2seq | end of epoch 37\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 15:11:55 | INFO | hw5.seq2seq | training loss: 3.2418\n",
            "2023-04-14 15:11:55 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 15:12:21 | INFO | hw5.seq2seq | example source: it releases cortisol that raises your heart rate , it modulates adrenaline levels and it clouds your thinking .\n",
            "2023-04-14 15:12:21 | INFO | hw5.seq2seq | example hypothesis: 它會釋放皮質醇 , 提高你的心跳 , 它會調節腎上腺素的濃度 , 會讓你的思考 。\n",
            "2023-04-14 15:12:21 | INFO | hw5.seq2seq | example reference: 我知道它會釋出皮質醇 , 增加你的心跳、調解腎上腺素、並讓你思緒渾沌不清 。\n",
            "2023-04-14 15:12:21 | INFO | hw5.seq2seq | validation loss:\t3.3209\n",
            "2023-04-14 15:12:21 | INFO | hw5.seq2seq | BLEU = 25.68 59.1/33.7/20.2/12.8 (BP = 0.957 ratio = 0.958 hyp_len = 105814 ref_len = 110430)\n",
            "2023-04-14 15:12:22 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint38.pt\n",
            "2023-04-14 15:12:22 | INFO | hw5.seq2seq | end of epoch 38\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 15:15:59 | INFO | hw5.seq2seq | training loss: 3.2378\n",
            "2023-04-14 15:15:59 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 15:16:25 | INFO | hw5.seq2seq | example source: it's not true . how could it be ?\n",
            "2023-04-14 15:16:25 | INFO | hw5.seq2seq | example hypothesis: 這不是真的 。 怎麼可能呢 ?\n",
            "2023-04-14 15:16:25 | INFO | hw5.seq2seq | example reference: 這怎麼可能呢 ? 這是完全錯誤的 。\n",
            "2023-04-14 15:16:25 | INFO | hw5.seq2seq | validation loss:\t3.3252\n",
            "2023-04-14 15:16:25 | INFO | hw5.seq2seq | BLEU = 25.47 59.3/33.8/20.4/12.9 (BP = 0.946 ratio = 0.947 hyp_len = 104603 ref_len = 110430)\n",
            "2023-04-14 15:16:25 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint39.pt\n",
            "2023-04-14 15:16:25 | INFO | hw5.seq2seq | end of epoch 39\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 15:20:01 | INFO | hw5.seq2seq | training loss: 3.2331\n",
            "2023-04-14 15:20:01 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 15:20:28 | INFO | hw5.seq2seq | example source: if you made that decision in 1965 , the down side of that is the next year we have the cultural revolution .\n",
            "2023-04-14 15:20:28 | INFO | hw5.seq2seq | example hypothesis: 假如你在1965年做了那個決定 , 下一年我們有了文化革命 。\n",
            "2023-04-14 15:20:28 | INFO | hw5.seq2seq | example reference: 如果你在1965年作出了這個決定 , 弊處是第二年爆發了文化大革命 。\n",
            "2023-04-14 15:20:28 | INFO | hw5.seq2seq | validation loss:\t3.3298\n",
            "2023-04-14 15:20:28 | INFO | hw5.seq2seq | BLEU = 25.81 59.0/33.8/20.5/13.0 (BP = 0.956 ratio = 0.956 hyp_len = 105624 ref_len = 110430)\n",
            "2023-04-14 15:20:28 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint40.pt\n",
            "2023-04-14 15:20:28 | INFO | hw5.seq2seq | end of epoch 40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 15:24:04 | INFO | hw5.seq2seq | training loss: 3.2297\n",
            "2023-04-14 15:24:04 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 15:24:31 | INFO | hw5.seq2seq | example source: and an especially important challenge that i've had to face is the great shortage of mental health professionals , such as psychiatrists and psychologists , particularly in the developing world .\n",
            "2023-04-14 15:24:31 | INFO | hw5.seq2seq | example hypothesis: 我所面臨的重大挑戰是心理健康專業人士的大缺陷 , 例如精神科醫生和心理學家 , 尤其是在發展中國家 。\n",
            "2023-04-14 15:24:31 | INFO | hw5.seq2seq | example reference: 而我們所要面對的一個特別重要的挑戰就是心理衛生專業人員的嚴重不足例如精神病學家與心理學家特別是在開發中世界\n",
            "2023-04-14 15:24:31 | INFO | hw5.seq2seq | validation loss:\t3.3358\n",
            "2023-04-14 15:24:31 | INFO | hw5.seq2seq | BLEU = 25.47 59.3/33.8/20.3/12.8 (BP = 0.948 ratio = 0.949 hyp_len = 104834 ref_len = 110430)\n",
            "2023-04-14 15:24:31 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint41.pt\n",
            "2023-04-14 15:24:31 | INFO | hw5.seq2seq | end of epoch 41\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 15:28:08 | INFO | hw5.seq2seq | training loss: 3.2269\n",
            "2023-04-14 15:28:08 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 15:28:35 | INFO | hw5.seq2seq | example source: if you ask men why they did a good job , they'll say , \" i'm awesome .\n",
            "2023-04-14 15:28:35 | INFO | hw5.seq2seq | example hypothesis: 如果你問男人為什麼他們做得很好 , 他們會說: 「 我很棒 。\n",
            "2023-04-14 15:28:35 | INFO | hw5.seq2seq | example reference: 如果你問男人 , 為什麼他們的工作做得不錯 , 他們會說 , \" 我棒極了 。\n",
            "2023-04-14 15:28:35 | INFO | hw5.seq2seq | validation loss:\t3.3160\n",
            "2023-04-14 15:28:35 | INFO | hw5.seq2seq | BLEU = 25.45 59.1/33.7/20.3/12.9 (BP = 0.947 ratio = 0.949 hyp_len = 104778 ref_len = 110430)\n",
            "2023-04-14 15:28:35 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint42.pt\n",
            "2023-04-14 15:28:35 | INFO | hw5.seq2seq | end of epoch 42\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 15:32:12 | INFO | hw5.seq2seq | training loss: 3.2229\n",
            "2023-04-14 15:32:12 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 15:32:39 | INFO | hw5.seq2seq | example source: this is a brain from a lamprey eel .\n",
            "2023-04-14 15:32:39 | INFO | hw5.seq2seq | example hypothesis: 這是來自一個燈光eel的大腦 。\n",
            "2023-04-14 15:32:39 | INFO | hw5.seq2seq | example reference: 這是一個從鰻魚取出的大腦\n",
            "2023-04-14 15:32:39 | INFO | hw5.seq2seq | validation loss:\t3.3234\n",
            "2023-04-14 15:32:39 | INFO | hw5.seq2seq | BLEU = 25.92 58.5/33.4/20.1/12.7 (BP = 0.974 ratio = 0.975 hyp_len = 107625 ref_len = 110430)\n",
            "2023-04-14 15:32:40 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint43.pt\n",
            "2023-04-14 15:32:40 | INFO | hw5.seq2seq | end of epoch 43\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 15:36:15 | INFO | hw5.seq2seq | training loss: 3.2184\n",
            "2023-04-14 15:36:15 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 15:36:42 | INFO | hw5.seq2seq | example source: and this was something that was really brought home to me a year ago when i found out i was pregnant and the first scan revealed that my baby had a birth defect associated with exposure to estrogenic chemicals in the womb and the second scan revealed no heartbeat .\n",
            "2023-04-14 15:36:42 | INFO | hw5.seq2seq | example hypothesis: 一年前 , 我發現我懷孕了 , 第一次掃描顯示 , 我的嬰兒出生缺陷與子宮內的雌激素化學物質和第二次掃描完全沒有心跳 。\n",
            "2023-04-14 15:36:42 | INFO | hw5.seq2seq | example reference: 一年前發生在我身上的事 , 讓我更加如此確信 。 當時我懷孕了 , 第一次的掃描檢查就發現 , 我的寶寶有先天性缺陷 。 這和我子宮中含有雌激素化學物質有關 。 第二次掃描時 , 胎兒已經沒有心跳 ,\n",
            "2023-04-14 15:36:42 | INFO | hw5.seq2seq | validation loss:\t3.3173\n",
            "2023-04-14 15:36:42 | INFO | hw5.seq2seq | BLEU = 25.47 59.5/33.9/20.4/12.9 (BP = 0.943 ratio = 0.944 hyp_len = 104298 ref_len = 110430)\n",
            "2023-04-14 15:36:42 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint44.pt\n",
            "2023-04-14 15:36:42 | INFO | hw5.seq2seq | end of epoch 44\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 15:40:19 | INFO | hw5.seq2seq | training loss: 3.2146\n",
            "2023-04-14 15:40:19 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 15:40:46 | INFO | hw5.seq2seq | example source: so one meeting tends to lead to another meeting , which leads to another meeting .\n",
            "2023-04-14 15:40:46 | INFO | hw5.seq2seq | example hypothesis: 所以一個會議會導致另一場會議 , 會議導致另一場會議 。\n",
            "2023-04-14 15:40:46 | INFO | hw5.seq2seq | example reference: 然後沒完沒了而且開會的人多\n",
            "2023-04-14 15:40:46 | INFO | hw5.seq2seq | validation loss:\t3.3181\n",
            "2023-04-14 15:40:46 | INFO | hw5.seq2seq | BLEU = 25.74 59.6/34.2/20.7/13.1 (BP = 0.944 ratio = 0.945 hyp_len = 104383 ref_len = 110430)\n",
            "2023-04-14 15:40:46 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint45.pt\n",
            "2023-04-14 15:40:46 | INFO | hw5.seq2seq | end of epoch 45\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 15:44:23 | INFO | hw5.seq2seq | training loss: 3.2135\n",
            "2023-04-14 15:44:23 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 15:44:50 | INFO | hw5.seq2seq | example source: how did he learn them ?\n",
            "2023-04-14 15:44:50 | INFO | hw5.seq2seq | example hypothesis: 他是怎麼學的 ?\n",
            "2023-04-14 15:44:50 | INFO | hw5.seq2seq | example reference: 他是怎麼學會的 ?\n",
            "2023-04-14 15:44:50 | INFO | hw5.seq2seq | validation loss:\t3.3205\n",
            "2023-04-14 15:44:50 | INFO | hw5.seq2seq | BLEU = 25.80 59.4/34.1/20.5/13.0 (BP = 0.951 ratio = 0.952 hyp_len = 105144 ref_len = 110430)\n",
            "2023-04-14 15:44:50 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint46.pt\n",
            "2023-04-14 15:44:50 | INFO | hw5.seq2seq | end of epoch 46\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 15:48:25 | INFO | hw5.seq2seq | training loss: 3.2087\n",
            "2023-04-14 15:48:25 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 15:48:50 | INFO | hw5.seq2seq | example source: we stay up all night focusing the lights , programming the lights , trying to find new ways to sculpt and carve light .\n",
            "2023-04-14 15:48:50 | INFO | hw5.seq2seq | example hypothesis: 我們一整晚都聚焦在燈光上 , 編寫燈光 , 試圖找出新的方法來雕塑和雕刻光 。\n",
            "2023-04-14 15:48:50 | INFO | hw5.seq2seq | example reference: 我們熬夜討論、設定燈光 , 試圖找到新的方式去雕塑、刻劃光 。\n",
            "2023-04-14 15:48:50 | INFO | hw5.seq2seq | validation loss:\t3.3249\n",
            "2023-04-14 15:48:50 | INFO | hw5.seq2seq | BLEU = 25.43 60.2/34.5/20.8/13.2 (BP = 0.925 ratio = 0.928 hyp_len = 102449 ref_len = 110430)\n",
            "2023-04-14 15:48:50 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint47.pt\n",
            "2023-04-14 15:48:50 | INFO | hw5.seq2seq | end of epoch 47\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 15:52:12 | INFO | hw5.seq2seq | training loss: 3.2059\n",
            "2023-04-14 15:52:12 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 15:52:37 | INFO | hw5.seq2seq | example source: and here's the thing: our kids are ready for this kind of work .\n",
            "2023-04-14 15:52:37 | INFO | hw5.seq2seq | example hypothesis: 重點是:我們的孩子已經準備好了這項工作了 。\n",
            "2023-04-14 15:52:37 | INFO | hw5.seq2seq | example reference: 重點是 , 我們的孩子已經準備好做這些事 。\n",
            "2023-04-14 15:52:37 | INFO | hw5.seq2seq | validation loss:\t3.3110\n",
            "2023-04-14 15:52:37 | INFO | hw5.seq2seq | BLEU = 25.82 59.2/34.0/20.5/13.0 (BP = 0.955 ratio = 0.956 hyp_len = 105537 ref_len = 110430)\n",
            "2023-04-14 15:52:37 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint48.pt\n",
            "2023-04-14 15:52:37 | INFO | hw5.seq2seq | end of epoch 48\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 15:55:59 | INFO | hw5.seq2seq | training loss: 3.2024\n",
            "2023-04-14 15:55:59 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 15:56:25 | INFO | hw5.seq2seq | example source: it forces air through a venturi force if there's no wind .\n",
            "2023-04-14 15:56:25 | INFO | hw5.seq2seq | example hypothesis: 如果沒有風 , 它會強迫空氣 。\n",
            "2023-04-14 15:56:25 | INFO | hw5.seq2seq | example reference: 它把空氣押進來 , 如果沒有風的話 , 就採用機器鼓風 。\n",
            "2023-04-14 15:56:25 | INFO | hw5.seq2seq | validation loss:\t3.3043\n",
            "2023-04-14 15:56:25 | INFO | hw5.seq2seq | BLEU = 25.76 59.0/33.8/20.4/13.0 (BP = 0.956 ratio = 0.957 hyp_len = 105669 ref_len = 110430)\n",
            "2023-04-14 15:56:25 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint49.pt\n",
            "2023-04-14 15:56:25 | INFO | hw5.seq2seq | end of epoch 49\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 15:59:47 | INFO | hw5.seq2seq | training loss: 3.2002\n",
            "2023-04-14 15:59:47 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 16:00:11 | INFO | hw5.seq2seq | example source: the queen of clubs !\n",
            "2023-04-14 16:00:11 | INFO | hw5.seq2seq | example hypothesis: 俱樂部的皇后 !\n",
            "2023-04-14 16:00:11 | INFO | hw5.seq2seq | example reference: 梅花后 !\n",
            "2023-04-14 16:00:11 | INFO | hw5.seq2seq | validation loss:\t3.3104\n",
            "2023-04-14 16:00:11 | INFO | hw5.seq2seq | BLEU = 25.71 59.8/34.4/20.8/13.2 (BP = 0.937 ratio = 0.939 hyp_len = 103642 ref_len = 110430)\n",
            "2023-04-14 16:00:12 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint50.pt\n",
            "2023-04-14 16:00:12 | INFO | hw5.seq2seq | end of epoch 50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 16:03:33 | INFO | hw5.seq2seq | training loss: 3.1975\n",
            "2023-04-14 16:03:33 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 16:03:58 | INFO | hw5.seq2seq | example source: most of them are in other buildings not designed as schools .\n",
            "2023-04-14 16:03:58 | INFO | hw5.seq2seq | example hypothesis: 大部分在其他的建築物裡沒有被設計成學校 。\n",
            "2023-04-14 16:03:58 | INFO | hw5.seq2seq | example reference: 其他則隱身於非學校的場地\n",
            "2023-04-14 16:03:58 | INFO | hw5.seq2seq | validation loss:\t3.3065\n",
            "2023-04-14 16:03:58 | INFO | hw5.seq2seq | BLEU = 25.66 59.2/33.8/20.3/12.9 (BP = 0.954 ratio = 0.955 hyp_len = 105432 ref_len = 110430)\n",
            "2023-04-14 16:03:58 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint51.pt\n",
            "2023-04-14 16:03:58 | INFO | hw5.seq2seq | end of epoch 51\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 16:07:20 | INFO | hw5.seq2seq | training loss: 3.1943\n",
            "2023-04-14 16:07:20 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 16:07:45 | INFO | hw5.seq2seq | example source: in fact , it's possible that reflecting just one or two percent more sunlight from the atmosphere could offset two degrees celsius or more of warming .\n",
            "2023-04-14 16:07:45 | INFO | hw5.seq2seq | example hypothesis: 事實上 , 只要反射出大氣中的1%或2%的陽光 , 就能抵禦攝氏2度或更暖化 。\n",
            "2023-04-14 16:07:45 | INFO | hw5.seq2seq | example reference: 事實上 , 有可能 , 只要從大氣再多反射1%或2%的陽光 , 就能抵消掉攝氏兩度以上的暖化 。\n",
            "2023-04-14 16:07:45 | INFO | hw5.seq2seq | validation loss:\t3.3143\n",
            "2023-04-14 16:07:45 | INFO | hw5.seq2seq | BLEU = 25.33 60.1/34.5/20.8/13.2 (BP = 0.922 ratio = 0.925 hyp_len = 102182 ref_len = 110430)\n",
            "2023-04-14 16:07:45 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint52.pt\n",
            "2023-04-14 16:07:45 | INFO | hw5.seq2seq | end of epoch 52\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 16:11:08 | INFO | hw5.seq2seq | training loss: 3.1936\n",
            "2023-04-14 16:11:08 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 16:11:35 | INFO | hw5.seq2seq | example source: thank you .\n",
            "2023-04-14 16:11:35 | INFO | hw5.seq2seq | example hypothesis: 謝謝\n",
            "2023-04-14 16:11:35 | INFO | hw5.seq2seq | example reference: 謝謝各位 。\n",
            "2023-04-14 16:11:35 | INFO | hw5.seq2seq | validation loss:\t3.3196\n",
            "2023-04-14 16:11:35 | INFO | hw5.seq2seq | BLEU = 25.60 60.2/34.6/20.9/13.4 (BP = 0.926 ratio = 0.929 hyp_len = 102590 ref_len = 110430)\n",
            "2023-04-14 16:11:35 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint53.pt\n",
            "2023-04-14 16:11:35 | INFO | hw5.seq2seq | end of epoch 53\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 16:15:05 | INFO | hw5.seq2seq | training loss: 3.1896\n",
            "2023-04-14 16:15:05 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 16:15:31 | INFO | hw5.seq2seq | example source: yes , there's issues about how money should be distributed , and that's still being refigured out .\n",
            "2023-04-14 16:15:31 | INFO | hw5.seq2seq | example hypothesis: 是的 , 有關於金錢如何分配的問題 , 這仍然被改造出來 。\n",
            "2023-04-14 16:15:31 | INFO | hw5.seq2seq | example reference: 是的 , 我們有資金分配的問題我們正在重新估算\n",
            "2023-04-14 16:15:31 | INFO | hw5.seq2seq | validation loss:\t3.3132\n",
            "2023-04-14 16:15:31 | INFO | hw5.seq2seq | BLEU = 25.46 60.6/34.9/21.1/13.4 (BP = 0.916 ratio = 0.919 hyp_len = 101473 ref_len = 110430)\n",
            "2023-04-14 16:15:31 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint54.pt\n",
            "2023-04-14 16:15:31 | INFO | hw5.seq2seq | end of epoch 54\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 16:19:07 | INFO | hw5.seq2seq | training loss: 3.1878\n",
            "2023-04-14 16:19:07 | INFO | hw5.seq2seq | begin validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-14 16:19:33 | INFO | hw5.seq2seq | example source: therefore , in this context , of course , it makes sense to dedicate all this time to spelling .\n",
            "2023-04-14 16:19:33 | INFO | hw5.seq2seq | example hypothesis: 因此 , 在這個情境中 , 當然 , 拼字是合理的 。\n",
            "2023-04-14 16:19:33 | INFO | hw5.seq2seq | example reference: 因此 , 在這種情境下 , 當然 , 把所有的時間花在拼字上是合理的 。\n",
            "2023-04-14 16:19:33 | INFO | hw5.seq2seq | validation loss:\t3.3042\n",
            "2023-04-14 16:19:33 | INFO | hw5.seq2seq | BLEU = 25.84 59.2/33.9/20.5/13.0 (BP = 0.956 ratio = 0.957 hyp_len = 105654 ref_len = 110430)\n",
            "2023-04-14 16:19:33 | INFO | hw5.seq2seq | saved epoch checkpoint: c:\\Users\\william\\Desktop\\graddescope\\checkpoints\\transformer/checkpoint55.pt\n",
            "2023-04-14 16:19:33 | INFO | hw5.seq2seq | end of epoch 55\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train epoch 56:  30%|██▉       | 238/797 [01:05<02:21,  3.96it/s, loss=3.21]"
          ]
        }
      ],
      "source": [
        "epoch_itr = load_data_iterator(task, \"train\", config.start_epoch, config.max_tokens, config.num_workers)\n",
        "try_load_checkpoint(model, optimizer, name=config.resume)\n",
        "while epoch_itr.next_epoch_idx <= config.max_epoch:\n",
        "    # train for one epoch\n",
        "    train_one_epoch(epoch_itr, model, task, criterion, optimizer, config.accum_steps)\n",
        "    stats = validate_and_save(model, task, criterion, optimizer, epoch=epoch_itr.epoch)\n",
        "    logger.info(\"end of epoch {}\".format(epoch_itr.epoch))    \n",
        "    epoch_itr = load_data_iterator(task, \"train\", epoch_itr.next_epoch_idx, config.max_tokens, config.num_workers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyjRwllxPjtf"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N70Gc6smPi1d"
      },
      "outputs": [],
      "source": [
        "# averaging a few checkpoints can have a similar effect to ensemble\n",
        "checkdir=config.savedir\n",
        "!python C:/Users/william/fairseq/scripts/average_checkpoints.py \\\n",
        "--inputs {checkdir} \\\n",
        "--num-epoch-checkpoints 5 \\\n",
        "--output {checkdir}/avg_last_5_checkpoint.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAGMiun8PnZy"
      },
      "source": [
        "## Confirm model weights used to generate submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvRdivVUPnsU"
      },
      "outputs": [],
      "source": [
        "# checkpoint_last.pt : latest epoch\n",
        "# checkpoint_best.pt : highest validation bleu\n",
        "# avg_last_5_checkpoint.pt: the average of last 5 epochs\n",
        "try_load_checkpoint(model, name=\"avg_last_5_checkpoint.pt\")\n",
        "validate(model, task, criterion, log_to_wandb=False)\n",
        "None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioAIflXpPsxt"
      },
      "source": [
        "## Generate Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYMxA8FlPtIq"
      },
      "outputs": [],
      "source": [
        "def generate_prediction(model, task, split=\"test\", outfile=\"./prediction-2.txt\"):    \n",
        "    task.load_dataset(split=split, epoch=1)\n",
        "    itr = load_data_iterator(task, split, 1, config.max_tokens, config.num_workers).next_epoch_itr(shuffle=False)\n",
        "    \n",
        "    idxs = []\n",
        "    hyps = []\n",
        "\n",
        "    model.eval()\n",
        "    progress = tqdm.tqdm(itr, desc=f\"prediction\")\n",
        "    with torch.no_grad():\n",
        "        for i, sample in enumerate(progress):\n",
        "            # validation loss\n",
        "            sample = utils.move_to_cuda(sample, device=device)\n",
        "\n",
        "            # do inference\n",
        "            s, h, r = inference_step(sample, model)\n",
        "            \n",
        "            hyps.extend(h)\n",
        "            idxs.extend(list(sample['id']))\n",
        "            \n",
        "    # sort based on the order before preprocess\n",
        "    hyps = [x for _,x in sorted(zip(idxs,hyps))]\n",
        "    \n",
        "    with open(outfile, \"w\") as f:\n",
        "        for h in hyps:\n",
        "            f.write(h+\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Le4RFWXxjmm0"
      },
      "outputs": [],
      "source": [
        "generate_prediction(model, task)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# gradescope1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"KMP_DUPLICATE_LTB_OK\"] = \"TRUE\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.nn.functional import cosine_similarity as cs\n",
        "pos_emb = model.decoder.embed_positions.weights.cpu().detach()\n",
        "print(pos_emb.size())\n",
        "ret = cs(pos_emb.unsqueeze(1), pos_emb, dim = 2)\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.matshow(ret)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# gradescope2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "gnorm_list = []\n",
        "with open('gnorm.txt', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        if line[0].isdigit():\n",
        "            gnorm_list.append(round(float(line[::-1]), 3))\n",
        "        else:\n",
        "            gnorm_list.append(-1)\n",
        "    print(gnorm_list[:100])\n",
        "\n",
        "# plt.plot(range(1, len(gnorms)+1, gnorms))\n",
        "# plt.plot(range(1, len(gnorms)+1), [config.clip_norm] * len(gnorms), \"-\")\n",
        "plt.plot([i for i in range(len(gnorm_list))], gnorm_list)\n",
        "plt.title('Grad norm v.s. step')\n",
        "plt.xlabel('step')\n",
        "plt.ylabel('Grad norm')\n",
        "plt.show"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvenyi6BPwnD"
      },
      "outputs": [],
      "source": [
        "raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z0cJE-wPzaU"
      },
      "source": [
        "# Back-translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-7uPJ2CP0sm"
      },
      "source": [
        "## Train a backward translation model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppGHjg2ZP3sV"
      },
      "source": [
        "1. Switch the source_lang and target_lang in **config** \n",
        "2. Change the savedir in **config** (eg. \"./checkpoints/transformer-back\")\n",
        "3. Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waTGz29UP6WI"
      },
      "source": [
        "## Generate synthetic data with backward model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIeTsPexP8FL"
      },
      "source": [
        "### Download monolingual data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7N4QlsbP8fh"
      },
      "outputs": [],
      "source": [
        "# mono_dataset_name = 'mono'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "396saD9-QBPY"
      },
      "outputs": [],
      "source": [
        "# mono_prefix = Path(data_dir).absolute() / mono_dataset_name\n",
        "# mono_prefix.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# urls = (\n",
        "#     \"https://github.com/figisiwirf/ml2023-hw5-dataset/releases/download/v1.0.1/ted_zh_corpus.deduped.gz\",\n",
        "# )\n",
        "# file_names = (\n",
        "#     'ted_zh_corpus.deduped.gz',\n",
        "# )\n",
        "\n",
        "# for u, f in zip(urls, file_names):\n",
        "#     path = mono_prefix/f\n",
        "#     if not path.exists():\n",
        "#         !wget {u} -O {path}\n",
        "#     else:\n",
        "#         print(f'{f} is exist, skip downloading')\n",
        "#     if path.suffix == \".tgz\":\n",
        "#         !tar -xvf {path} -C {prefix}\n",
        "#     elif path.suffix == \".zip\":\n",
        "#         !unzip -o {path} -d {prefix}\n",
        "#     elif path.suffix == \".gz\":\n",
        "#         !gzip -fkd {path}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOVQRHzGQU4-"
      },
      "source": [
        "### TODO: clean corpus\n",
        "\n",
        "1. remove sentences that are too long or too short\n",
        "2. unify punctuation\n",
        "\n",
        "hint: you can use clean_s() defined above to do this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIYmxfUOQSov"
      },
      "outputs": [],
      "source": [
        "# mono_prefix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def clean_mono_corpus(mono_prefix, l1, l2, max_len=1000, min_len=1):\n",
        "#     if Path(f'{mono_prefix}/ted_zh_corpus.deduped.clean.{l1}').exists() and Path(f'{mono_prefix}/ted_zh_corpus.deduped.clean.{l2}').exists():\n",
        "#         print(f'{mono_prefix}/ted_zh_corpus.deduped.clean.{l1} & {l2} exists. skipping clean.')\n",
        "#         return\n",
        "#     with open(f'{mono_prefix}/ted_zh_corpus.deduped', 'r') as l1_in_f:\n",
        "#         with open(f'{mono_prefix}/ted_zh_corpus.deduped.clean.{l1}', 'w') as l1_out_f:\n",
        "#             with open(f'{mono_prefix}/ted_zh_corpus.deduped.clean.{l2}', 'w') as l2_out_f:\n",
        "#                 for s1 in l1_in_f:\n",
        "#                     s1 = s1.strip()\n",
        "#                     s1 = clean_s(s1, l1)\n",
        "#                     s1_len = len_s(s1, l1)\n",
        "#                     if min_len > 0: # remove short sentence\n",
        "#                         if s1_len < min_len:\n",
        "#                             continue\n",
        "#                     if max_len > 0: # remove long sentence\n",
        "#                         if s1_len > max_len:\n",
        "#                             continue\n",
        "#                     print(s1, file=l1_out_f)\n",
        "#                     print('.', file=l2_out_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# clean_mono_corpus(mono_prefix, 'zh','en')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# !head {data_prefix+'.clean.'+'zh'} -n 5\n",
        "# !head {data_prefix+'.clean.'+'en'} -n 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jegH0bvMQVmR"
      },
      "source": [
        "### TODO: Subword Units\n",
        "\n",
        "Use the spm model of the backward model to tokenize the data into subword units\n",
        "\n",
        "hint: spm model is located at DATA/raw-data/\\[dataset\\]/spm\\[vocab_num\\].model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqgR4uUMQZGY"
      },
      "outputs": [],
      "source": [
        "# for lang in ['zh','en']:\n",
        "#     out_path = mono_prefix/f'mono.tok.{lang}'\n",
        "#     if out_path.exists():\n",
        "#         print(f\"{out_path} exists. skipping spm_encode.\")\n",
        "#     else:\n",
        "#         with open(mono_prefix/f'mono.tok.{lang}', 'w') as out_f:\n",
        "#             with open(mono_prefix/f'ted_zh_corpus.deduped.clean.{lang}', 'r') as in_f:\n",
        "#                 for line in in_f:\n",
        "#                     line = line.strip()\n",
        "#                     tok = spm_model.encode(line, out_type=str)\n",
        "#                     print(' '.join(tok), file=out_f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a65glBVXQZiE"
      },
      "source": [
        "### Binarize\n",
        "\n",
        "use fairseq to binarize data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b803qA5aQaEu"
      },
      "outputs": [],
      "source": [
        "# binpath = Path('./DATA/data-bin', mono_dataset_name)\n",
        "# src_dict_file = './DATA/data-bin/ted2020/dict.en.txt'\n",
        "# tgt_dict_file = src_dict_file\n",
        "# monopref = str(mono_prefix/\"mono.tok\") # whatever filepath you get after applying subword tokenization\n",
        "# if binpath.exists():\n",
        "#     print(binpath, \"exists, will not overwrite!\")\n",
        "# else:\n",
        "#     !python -m fairseq_cli.preprocess\\\n",
        "#         --source-lang 'zh'\\\n",
        "#         --target-lang 'en'\\\n",
        "#         --trainpref {monopref}\\\n",
        "#         --destdir {binpath}\\\n",
        "#         --srcdict {src_dict_file}\\\n",
        "#         --tgtdict {tgt_dict_file}\\\n",
        "#         --workers 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smA0JraEQdxz"
      },
      "source": [
        "### TODO: Generate synthetic data with backward model\n",
        "\n",
        "Add binarized monolingual data to the original data directory, and name it with \"split_name\"\n",
        "\n",
        "ex. ./DATA/data-bin/ted2020/\\[split_name\\].zh-en.\\[\"en\", \"zh\"\\].\\[\"bin\", \"idx\"\\]\n",
        "\n",
        "then you can use 'generate_prediction(model, task, split=\"split_name\")' to generate translation prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvaOVHeoQfkB"
      },
      "outputs": [],
      "source": [
        "# # Add binarized monolingual data to the original data directory, and name it with \"split_name\"\n",
        "# # ex. ./DATA/data-bin/ted2020/\\[split_name\\].zh-en.\\[\"en\", \"zh\"\\].\\[\"bin\", \"idx\"\\]\n",
        "# !cp ./DATA/data-bin/mono/train.zh-en.zh.bin ./DATA/data-bin/ted2020/mono.zh-en.zh.bin\n",
        "# !cp ./DATA/data-bin/mono/train.zh-en.zh.idx ./DATA/data-bin/ted2020/mono.zh-en.zh.idx\n",
        "# !cp ./DATA/data-bin/mono/train.zh-en.en.bin ./DATA/data-bin/ted2020/mono.zh-en.en.bin\n",
        "# !cp ./DATA/data-bin/mono/train.zh-en.en.idx ./DATA/data-bin/ted2020/mono.zh-en.en.idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # hint: 用反向模型在 split='mono' 上進行預測，生成 prediction_file\n",
        "# generate_prediction(model, task, split=\"mono\", outfile=\"./DATA/rawdata/mono/mono_prediction.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFEkxPu-Qhlc"
      },
      "outputs": [],
      "source": [
        "# hint: do prediction on split='mono' to create prediction_file\n",
        "# generate_prediction( ... ,split=... ,outfile=... )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn4XeawpQjLk"
      },
      "source": [
        "### TODO: Create new dataset\n",
        "\n",
        "1. Combine the prediction data with monolingual data\n",
        "2. Use the original spm model to tokenize data into Subword Units\n",
        "3. Binarize data with fairseq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3R35JTaTQjkm"
      },
      "outputs": [],
      "source": [
        "# Combine prediction_file (.en) and mono.zh (.zh) into a new dataset.\n",
        "# \n",
        "# hint: tokenize prediction_file with the spm model\n",
        "# spm_model.encode(line, out_type=str)\n",
        "# output: ./DATA/rawdata/mono/mono.tok.en & mono.tok.zh\n",
        "#\n",
        "# hint: use fairseq to binarize these two files again\n",
        "# binpath = Path('./DATA/data-bin/synthetic')\n",
        "# src_dict_file = './DATA/data-bin/ted2020/dict.en.txt'\n",
        "# tgt_dict_file = src_dict_file\n",
        "# monopref = ./DATA/rawdata/mono/mono.tok # or whatever path after applying subword tokenization, w/o the suffix (.zh/.en)\n",
        "# if binpath.exists():\n",
        "#     print(binpath, \"exists, will not overwrite!\")\n",
        "# else:\n",
        "#     !python -m fairseq_cli.preprocess\\\n",
        "#         --source-lang 'zh'\\\n",
        "#         --target-lang 'en'\\\n",
        "#         --trainpref {monopref}\\\n",
        "#         --destdir {binpath}\\\n",
        "#         --srcdict {src_dict_file}\\\n",
        "#         --tgtdict {tgt_dict_file}\\\n",
        "#         --workers 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # 合併剛剛生成的 prediction_file (.en) 以及中文 mono.zh (.zh)\n",
        "# # \n",
        "# # hint: 在此用剛剛的 spm model 對 prediction_file 進行切斷詞\n",
        "# # spm_model.encode(line, out_type=str)\n",
        "# # output: ./DATA/rawdata/mono/mono.tok.en & mono.tok.zh\n",
        "# #\n",
        "# with open(mono_prefix/f'mono.tok.en', 'w') as out_f:\n",
        "#     with open('./DATA/rawdata/mono/mono_prediction.txt', 'r') as in_f:\n",
        "#         for line in in_f:\n",
        "#             line = line.strip()\n",
        "#             tok = spm_model.encode(line, out_type=str)\n",
        "#             print(' '.join(tok), file=out_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # hint: 在此用 fairseq 把這些檔案再 binarize\n",
        "# binpath = Path('./DATA/data-bin/synthetic')\n",
        "# src_dict_file = './DATA/data-bin/ted2020/dict.en.txt'\n",
        "# tgt_dict_file = src_dict_file\n",
        "# monopref = Path('./DATA/rawdata/mono/mono.tok') # or whatever path after applying subword tokenization, w/o the suffix (.zh/.en)\n",
        "# if binpath.exists():\n",
        "#     print(binpath, \"exists, will not overwrite!\")\n",
        "# else:\n",
        "#     !python -m fairseq_cli.preprocess\\\n",
        "#          --source-lang 'zh'\\\n",
        "#          --target-lang 'en'\\\n",
        "#          --trainpref {monopref}\\\n",
        "#          --destdir {binpath}\\\n",
        "#          --srcdict {src_dict_file}\\\n",
        "#          --tgtdict {tgt_dict_file}\\\n",
        "#          --workers 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSkse1tyQnsR"
      },
      "outputs": [],
      "source": [
        "# # create a new dataset from all the files prepared above\n",
        "# !cp -r ./DATA/data-bin/ted2020/ ./DATA/data-bin/ted2020_with_mono/\n",
        "\n",
        "# !cp ./DATA/data-bin/synthetic/train.zh-en.zh.bin ./DATA/data-bin/ted2020_with_mono/train1.en-zh.zh.bin\n",
        "# !cp ./DATA/data-bin/synthetic/train.zh-en.zh.idx ./DATA/data-bin/ted2020_with_mono/train1.en-zh.zh.idx\n",
        "# !cp ./DATA/data-bin/synthetic/train.zh-en.en.bin ./DATA/data-bin/ted2020_with_mono/train1.en-zh.en.bin\n",
        "# !cp ./DATA/data-bin/synthetic/train.zh-en.en.idx ./DATA/data-bin/ted2020_with_mono/train1.en-zh.en.idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# config = Namespace(\n",
        "#     datadir = \"./DATA/data-bin/ted2020_with_mono\",\n",
        "#     savedir = \"/content/drive/MyDrive/ML2021-hw5/checkpoints/transformer-big\",\n",
        "#     source_lang = \"en\",\n",
        "#     target_lang = \"zh\",\n",
        "    \n",
        "#     # cpu threads when fetching & processing data.\n",
        "#     num_workers=2,  \n",
        "#     # batch size in terms of tokens. gradient accumulation increases the effective batchsize.\n",
        "#     max_tokens=4096,\n",
        "#     accum_steps=4,\n",
        "    \n",
        "#     # the lr s calculated from Noam lr scheduler. you can tune the maximum lr by this factor.\n",
        "#     lr_factor=2.,\n",
        "#     lr_warmup=4000,\n",
        "    \n",
        "#     # clipping gradient norm helps alleviate gradient exploding\n",
        "#     clip_norm=1.0,\n",
        "    \n",
        "#     # maximum epochs for training\n",
        "#     max_epoch=35,\n",
        "#     start_epoch=1,\n",
        "    \n",
        "#     # beam size for beam search\n",
        "#     beam=5, \n",
        "#     # generate sequences of maximum length ax + b, where x is the source length\n",
        "#     max_len_a=1.2, \n",
        "#     max_len_b=10,\n",
        "#     # when decoding, post process sentence by removing sentencepiece symbols.\n",
        "#     post_process = \"sentencepiece\",\n",
        "    \n",
        "#     # checkpoints\n",
        "#     keep_last_epochs=15,\n",
        "#     resume=None, # if resume from checkpoint name (under config.savedir)\n",
        "    \n",
        "#     # logging\n",
        "#     use_wandb=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ## setup task\n",
        "# task_cfg = TranslationConfig(\n",
        "#     data=config.datadir,\n",
        "#     source_lang=config.source_lang,\n",
        "#     target_lang=config.target_lang,\n",
        "#     train_subset=\"train\",\n",
        "#     required_seq_len_multiple=8,\n",
        "#     dataset_impl=\"mmap\",\n",
        "#     upsample_primary=1,\n",
        "# )\n",
        "# task = TranslationTask.setup_task(task_cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# logger.info(\"loading data for epoch 1\")\n",
        "# task.load_dataset(split=\"train\", epoch=1, combine=True) # combine if you have back-translation data.\n",
        "# task.load_dataset(split=\"valid\", epoch=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# sample = task.dataset(\"valid\")[1]\n",
        "# pprint.pprint(sample)\n",
        "# pprint.pprint(\n",
        "#     \"Source: \" + \\\n",
        "#     task.source_dictionary.string(\n",
        "#         sample['source'],\n",
        "#         config.post_process,\n",
        "#     )\n",
        "# )\n",
        "# pprint.pprint(\n",
        "#     \"Target: \" + \\\n",
        "#     task.target_dictionary.string(\n",
        "#         sample['target'],\n",
        "#         config.post_process,\n",
        "#     )\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# demo_epoch_obj = load_data_iterator(task, \"valid\", epoch=1, max_tokens=20, num_workers=1, cached=False)\n",
        "# demo_iter = demo_epoch_obj.next_epoch_itr(shuffle=True)\n",
        "# sample = next(demo_iter)\n",
        "# sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # transformer-big\n",
        "# arch_args = Namespace(\n",
        "#     encoder_embed_dim=1024,\n",
        "#     encoder_ffn_embed_dim=4096,\n",
        "#     encoder_layers=6,\n",
        "#     decoder_embed_dim=1024,\n",
        "#     decoder_ffn_embed_dim=4096,\n",
        "#     decoder_layers=6,\n",
        "#     share_decoder_input_output_embed=True,\n",
        "#     dropout=0.3,\n",
        "# )\n",
        "\n",
        "# # # HINT: 補上Transformer用的參數\n",
        "# def add_transformer_args(args):\n",
        "#     args.encoder_attention_heads=16\n",
        "#     args.encoder_normalize_before=True\n",
        "    \n",
        "#     args.decoder_attention_heads=16\n",
        "#     args.decoder_normalize_before=True\n",
        "    \n",
        "#     args.activation_fn=\"relu\"\n",
        "#     args.max_source_positions=1024\n",
        "#     args.max_target_positions=1024\n",
        "    \n",
        "#     # 補上我們沒有設定的Transformer預設參數\n",
        "#     from fairseq.models.transformer import base_architecture \n",
        "#     base_architecture(arch_args)\n",
        "\n",
        "# add_transformer_args(arch_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model = build_model(arch_args, task)\n",
        "# logger.info(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # 把幾個 checkpoint 平均起來可以達到 ensemble 的效果\n",
        "# checkdir=config.savedir\n",
        "# !python ./fairseq/scripts/average_checkpoints.py \\\n",
        "# --inputs {checkdir} \\\n",
        "# --num-epoch-checkpoints 5 \\\n",
        "# --output {checkdir}/avg_last_5_checkpoint.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # checkpoint_last.pt : 最後一次檢驗的檔案\n",
        "# # checkpoint_best.pt : 檢驗 BLEU 最高的檔案\n",
        "# # avg_last_5_checkpoint.pt:　最5後個檔案平均\n",
        "# try_load_checkpoint(model, name=\"avg_last_5_checkpoint.pt\")\n",
        "# validate(model, task, criterion, log_to_wandb=False)\n",
        "# None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# generate_prediction(model, task, outfile=\".//prediction-2.txt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVdxVGO3QrSs"
      },
      "source": [
        "Created new dataset \"ted2020_with_mono\"\n",
        "\n",
        "1. Change the datadir in **config** (\"./DATA/data-bin/ted2020_with_mono\")\n",
        "2. Switch back the source_lang and target_lang in **config** (\"en\", \"zh\")\n",
        "2. Change the savedir in **config** (eg. \"./checkpoints/transformer-bt\")\n",
        "3. Train model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "z-m3IsoJrhmd"
      },
      "source": [
        "# References\n",
        "\n",
        "看這邊: https://github.com/pai4451/ML2021/blob/main/hw5/hw5.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rrfm6iLJQ0tS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "nKb4u67-sT_Z",
        "n1rwQysTsdJq",
        "59si_C0Wsms7",
        "OI46v1z7MotH",
        "6ZlE_1JnMv56",
        "UDAPmxjRNEEL",
        "ce5n4eS7NQNy",
        "rUB9f1WCNgMH",
        "VFJlkOMONsc6",
        "Gt1lX3DRO_yU",
        "BAGMiun8PnZy",
        "JOVQRHzGQU4-",
        "jegH0bvMQVmR",
        "a65glBVXQZiE",
        "smA0JraEQdxz",
        "Jn4XeawpQjLk",
        "z-m3IsoJrhmd"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
